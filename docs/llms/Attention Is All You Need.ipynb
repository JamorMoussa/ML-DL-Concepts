{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cbff0df-162f-4f17-8a2c-61c00429eef2",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/attention-is-all-you-need.png\" width=\"500\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213814b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623535c-5328-4138-9952-d99c269e7950",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In 2017, researchers from Google published a paper titled [**Attention Is All You Need**](https://arxiv.org/pdf/1706.03762), introducing a novel architecture called the **Transformer** in the context of machine translation tasks. This architecture has significantly influenced AI research and inspired many other applications.\n",
    "\n",
    "The **Transformer** is at the heart of modern **Large Language Models** (LLMs), the most famous of which is [**ChatGPT**](https://chat.openai.com/) from [**OpenAI**](https://openai.com/). **ChatGPT** is based on the **GPT** architecture, which stands for **Generative Pre-trained Transformer** and serves as an assistant chatbot. Another notable application of the **Transformer** architecture is **BERT**, described in the [**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**](https://arxiv.org/pdf/1810.04805) paper. The **BERT** model is used in many tasks like sentiment analysis, question answering (Q&A), and more.\n",
    "\n",
    "In this tutorial, we will delve deep into the [**Attention Is All You Need**](https://arxiv.org/pdf/1706.03762) paper, exploring the **Transformer** architecture in depth, from theory to code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581538a8-c785-4409-8c4f-71fa16bef40d",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/transformer-arch.png\" width=\"320\" />  \n",
    "  <figcaption> <b>Tranformer Architecture</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b5d0c-4af8-4f1b-9eb5-c20a25478842",
   "metadata": {},
   "source": [
    "## 01. What's Motivation behind Transformers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a263017-91ef-4144-9431-64632d8ae6b7",
   "metadata": {},
   "source": [
    "**Recurrent Neural Networks (RNNs)** were once the state-of-the-art for sequence-to-sequence modeling. They are used for handling sequence data such as language modeling, speech recognition, machine translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2831249-5424-47d1-8f53-e3208473fdd2",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/recurrent-net.png\" width=\"140\" />  \n",
    "  <figcaption> <b>Recurrent Neural Networks Architecture</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5614a94-0365-438b-a703-d0052d3bce38",
   "metadata": {},
   "source": [
    "**RNNs** are quite effective for handling small sequences. However, they lose efficiency when dealing with large sequences. To address this, **Long Short-Term Memory (LSTM)** and **Gated Recurrent Unit (GRU)**, which are variants of **RNNs**, were introduced to tackle the vanishing gradients issue that arises with longer sequences. Despite these advancements, all RNN architectures still suffer from several issues:\n",
    "\n",
    "- **RNNs are slow**: Due to their sequential nature, RNNs process inputs one at a time to generate the output. For example, to process the third token in a sequence, we must first process the first and second tokens. This sequential dependency makes both training and inference slow.\n",
    "\n",
    "- **Gradient issues**: **Vanishing Gradient** and **Exploding Gradient** are significant problems with RNNs. Although **LSTMs** and **GRUs** were designed to mitigate these issues, they remain unavoidable when dealing with long contexts and sequences.\n",
    "\n",
    "- **Context Representations**: The intermediate activation vectors passed from the previous hidden state $h_{t-1}$ to the current hidden state $h_t$ are meant to carry information from previous tokens. However, it is uncertain whether they accurately represent the context, especially with long dependencies where information tends to be lost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8ebb4-317a-490a-ac0c-fa44c78e751f",
   "metadata": {},
   "source": [
    "In the next sections, we will explore how **Transformers** address these issues using a clever trick called the **Attention Mechanism**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47efbb5-7b55-4670-8d47-d0593e4b5836",
   "metadata": {},
   "source": [
    "## 02. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1397f-5cc6-463a-970b-b119694ccca3",
   "metadata": {},
   "source": [
    "### 2.1 Understanding Tokenization in Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180baec-3d59-4526-8d8b-70389b6dddcb",
   "metadata": {},
   "source": [
    "First of all, before we dive into understanding what tokenization is, I want to mention that **Andrej Karpathy** created a great video on this topic.\n",
    "\n",
    "You can find it here [https://www.youtube.com/watch?v=zduSFxRajkE](https://www.youtube.com/watch?v=zduSFxRajkE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a056b-c1dc-4ff6-acc4-70ccf8c78775",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/tokenizer-andrej.png\"/>  \n",
    "  <figcaption> <b> Let's build the GPT Tokenizer </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba85a5-bca0-4fc0-bf30-961f511b7de5",
   "metadata": {},
   "source": [
    "#### What's Tokenization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb35f9-557f-43f8-94ff-4e9fc2d7d3dc",
   "metadata": {},
   "source": [
    "In simple terms, **Tokenization** is the process of breaking sequences into smaller **tokens**. These tokens can vary in size, ranging from single letters to sub-words, words, or even multiple words. Each token is then assigned an integer that represents its unique identifier, or `id`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead57a0-7b18-4a9f-a6d4-7a4cfd78ad5d",
   "metadata": {},
   "source": [
    "The **Tokenization** stage is very critical before plugging text sequences naively into the language models; without it, they won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554acb8-8bf0-42f5-9a5e-4313f7b62263",
   "metadata": {},
   "source": [
    "Actually, you can test the tokenization online by visiting the following website [tiktokenizer](https://tiktokenizer.vercel.app/?model=o200k_base). After entering some text, you can visually see that the sequence is broken into tokens on the right, with different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb3637-9f38-40e4-aa89-7a80f7ec4542",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/tokenizer-tiktoten.png\"/>  \n",
    "  <figcaption> <b>Tiktoken Vercel App</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26abef5d-d852-4b80-a693-504510fe6c4e",
   "metadata": {},
   "source": [
    "#### Why Tokenization is important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4649382f-960e-447f-b0a3-f484b3721e00",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be3481-6e0a-42f0-9621-36a065c6fe43",
   "metadata": {},
   "source": [
    "<div class=\"admonition question\" markdown>\n",
    "    <p class=\"admonition-title\">Question</p>\n",
    "    <p>\n",
    "    A very good question that can be asked at this moment is, <b>why don't we just consider words as tokens, separated by white space?</b>\n",
    "    </p>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c43ed5-caad-436a-8420-ff909cb2d27f",
   "metadata": {},
   "source": [
    "It's very convenient and logical to consider why complicate things by breaking words into sub-words, etc.\n",
    "\n",
    "In simple terms, if we consider tokens as just words separated by white space, we'll actually encounter a couple of issues:\n",
    "\n",
    "- Firstly, the text sequences we work with are not always written in high-quality format. For example, if the sequence contains something like `Attention isallyou need`, we would consider `isallyou` as one word, but it's actually three different words. Therefore, we need a clever way to tokenize the sequence.\n",
    "\n",
    "- Secondly, tokenization determines the vocabulary, which consists of unique tokens. If we consider tokens as just words, we end up with a very large vocabulary size, potentially in the order of **hundreds of thousands** or even **millions**. This can be challenging when dealing with language modeling. However, if we consider tokens as just characters, we end up with a vocabulary size of around a hundred. This is not sufficient for effective language modeling. Therefore, the solution is to find a trade-off between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f2c89-09a2-4932-9157-da39d71ae094",
   "metadata": {},
   "source": [
    "In the tutorial **Let's build the GPT Tokenizer**, **Andrej Karpathy** raises some issues with **LLMs** due to bad **Tokenization**, here are some of them:\n",
    "\n",
    "- Why can't LLM spell words?\n",
    "- Why can't LLM perform super simple string processing tasks like reversing a string?\n",
    "- Why is LLM worse at non-English languages (e.g. Japanese)?\n",
    "- Why is LLM bad at simple arithmetic?\n",
    "- Why did GPT-2 have more than necessary trouble coding in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd0abb-bced-4572-bc71-e3fb90697a31",
   "metadata": {},
   "source": [
    "#### Tokenization in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb7440-9e37-41dd-95aa-4ccf3fef33ee",
   "metadata": {},
   "source": [
    "In fact, there are many tokenizers available, and each LLM comes with its own tokenizer. For OpenAI, it uses [tiktoken](https://github.com/openai/tiktoken) for their GPT LLMs, which is based on the [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) algorithm. Google uses [SentencePiece](https://github.com/google/sentencepiece), an unsupervised text tokenizer.\n",
    "\n",
    "In this section, we're going to use a toy examples with **tiktoken**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379d041-3d39-483c-bbe6-1bb596a0fd72",
   "metadata": {},
   "source": [
    "First, let's install the `tiktoken` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0980262b-9926-4ed5-aec6-2f3102d7dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/61/b4/b80d1fe33015e782074e96bbbf4108ccd283b8deea86fb43c15d18b7c351/tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/moussa/.local/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/lib/python3/dist-packages (from tiktoken) (2.31.0)\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305f6cb-0a04-4e67-9b23-12e8c5221119",
   "metadata": {},
   "source": [
    "Next, import the library and load the tokenizer for `gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79289b4b-c12c-4f66-b320-d14de6c5aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ef0d8b-47a9-4f40-8a52-73a0dcfac9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1a9e9-42e7-4145-bb60-0b8c67af7502",
   "metadata": {},
   "source": [
    "Now, let's use the `gpt2` tokenizer to encode the text `hello everyone` and then decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3b264d1-cca0-4eba-8a76-987b209b07c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373, 2506]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = encoder.encode(\"hello everyone\")\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d303c470-b66f-4d91-81ac-bb69fd6f65cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ' everyone']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoder.decode([c]) for c in code]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd75bee-22b2-4af4-b7dd-a8008571f32a",
   "metadata": {},
   "source": [
    "### 2.2 Understanding Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960b8d9-5b31-4dec-ae93-fc73aedfc6fd",
   "metadata": {},
   "source": [
    "Nowadays, AI technologies are used no matter the data type and task you have. The reason behind it from my perspective, is **Embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441b46f-6c6a-41b4-884a-2949575038ae",
   "metadata": {},
   "source": [
    "**Embeddings** are numerical representations of real-world objects, and data. They are used to capture the **semantic** and **Syntatic relationships**, and then later these embeddings are used by ML/AI systems to learn more complex patterns, and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a38ac-8d4f-477e-9774-da2763a12e0b",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/vector-embeddings.png\" width=\"700\" />  \n",
    "  <figcaption> <b>Vector Embeddings</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ce67e-963b-41d4-af59-eeda0f799a09",
   "metadata": {},
   "source": [
    "Before, starting to dive into the details, Here are two blogs for further information.\n",
    "\n",
    "- **Amazon AWS** : [What Are Embeddings In Machine Learning?](https://aws.amazon.com/what-is/embeddings-in-machine-learning/).\n",
    "- **OpenAI**: [Introducing text and code embeddings](https://openai.com/index/introducing-text-and-code-embeddings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6ebea-1188-4741-9667-12b55fa6b514",
   "metadata": {},
   "source": [
    "#### Why are Embeddings important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909ec4e-0416-4f77-8fcd-52f2028b44bc",
   "metadata": {},
   "source": [
    "Well, the short answer is we cannot plug data, i.e. text, images, audio, etc, into machine learning algorithms as they are. Instead, a preprocessing stage is needed, in which data is converted into some numerical representations, **Embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95843a34-41ec-4f52-85b3-6ba01990f981",
   "metadata": {},
   "source": [
    "That means machine learning algorithms can be used for different purposes and handling data no matter their type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d310905-9308-4153-b508-0e72ae7e1704",
   "metadata": {},
   "source": [
    "Embeddings are used for many purposes, let's explore some of them.\n",
    "\n",
    "- **Dimensionality Reduction**: In that case, embeddings are used to encode and compress some high-dimension data into low-dimension space. Which helps a lot to reduce complexity, and makes algorithms fast in training and inference.\n",
    "\n",
    "- **Large Language Models**: Retrieve Augmented Generation **(RAG)** helps LLMs generate more facts by providing them with factual inputs, similar to the user's query, for this similarity we use embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2cbee-b8f9-4a52-805d-86d3f49d838c",
   "metadata": {},
   "source": [
    "For further information about **RAG**, I suggest checking a great tutorial [Local Retrieval Augmented Generation (RAG) from Scratch](https://www.youtube.com/watch?v=qN_2fnOPY-M) made by **Daniel Bourke**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a03118-54cf-496e-aab5-cbc8d4a9ad9c",
   "metadata": {},
   "source": [
    "#### How are Embeddings Created?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a331b9-0270-4931-b026-c16900662de5",
   "metadata": {},
   "source": [
    "Embeddings are created, by training Neural Networks to encode the information into dense representation in a multi-dimention space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08600684-7206-463e-b1c8-c23d6500460b",
   "metadata": {},
   "source": [
    "Let's explore some neural network architectures used for learning embeddings for text data.\n",
    "\n",
    "- **Word2Vec**: This architecture is proposed by Google in two versions, **Continuous Bag of Words (CBOW)** and **Skip-Gram**. With the CBOW method, embeddings are learned by performing classification tasks. Given a context size, called window size `w`, we predict the word in the middle.\n",
    "\n",
    "- **BERT**: is a Transformer-based architecture. It is trained on massive data to understand language by performing two tasks: **Language Masking** and **Next Sentence Prediction**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236f91d-db32-4439-9129-9d21d63940f7",
   "metadata": {},
   "source": [
    "A simple difference between **Word2Vec** and **BERT**, is that **Word2Vec** cannot distinguish contextual differences of the same word used to imply different meanings. On the other hand, **BERT** can change the embeddings of a token (word) depending on the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f206f2-5775-4423-8953-bd720cd38eef",
   "metadata": {},
   "source": [
    "#### Embeddings in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53d4f3-499a-4b41-bee6-33a9bad81807",
   "metadata": {},
   "source": [
    "Let's explore the word embeddings in code. First, we import `gensim` library will provide pre-trained word embeddings, like `glove-twitter-25` that encode each word into a dense vector of 25 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fd14f7-76d1-49f1-8273-1afafc744851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81295820-dc72-44a9-9f14-e75de04b8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c81088-2e70-4521-bbe8-14b91cfa51bf",
   "metadata": {},
   "source": [
    "After downloading and load the `glove-twitter-25` word embeddings model, let's find the most similar to word **king**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f16b542-5c81-4d42-a093-323963ea96bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.9337409734725952),\n",
       " ('queen', 0.9202421307563782),\n",
       " ('aka', 0.9176921844482422),\n",
       " ('lady', 0.9163240790367126),\n",
       " ('jack', 0.9147354364395142),\n",
       " (\"'s\", 0.9066898226737976),\n",
       " ('stone', 0.8982374668121338),\n",
       " ('mr.', 0.8919409513473511),\n",
       " ('the', 0.889343798160553),\n",
       " ('star', 0.8892088532447815)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd306a-1cff-43d5-ab95-c4d4ca5f8758",
   "metadata": {},
   "source": [
    "Beautiful, we can see that **prince** and **queen** are on top of the list. That shows how these vector representations capture the meaning of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d95b1c-4e80-4f9a-b273-6d469cfff7fa",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" markdown>\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "       We can see on the list that, <b>'s</b> is similar to the word <b>king</b> with <b>90.66%</b>. Is not make sense, right? \n",
    "    </p>\n",
    "    <p>\n",
    "        The raison is that the quality of the embeddings depends on the quality of data, the NNs architecture used and more, as you can notice that the data used for the training comes form the twitter. That explains why we get this results.\n",
    "    </p>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b8633-7c46-409f-9296-19acba19410c",
   "metadata": {},
   "source": [
    "Let's define the cosine similarity metric that measure the angle between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ee8a60-58be-4cfa-9b32-858217d6f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6c7f7-90d9-404a-bea2-4cbf2493c530",
   "metadata": {},
   "source": [
    "Let's have fun and play with some embeddings vectors, to replicate the famous equation `king = queen + man - woman` and see what we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "324eecb2-c8b5-48ac-8645-d64d153946dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "man = glove.get_vector(\"man\")\n",
    "woman = glove.get_vector(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7371754d-37cd-48ce-b930-f819218e7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "king = glove.get_vector(\"king\")\n",
    "queen = glove.get_vector(\"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05faec-30ae-46f1-8521-78323c214b6e",
   "metadata": {},
   "source": [
    "First, let's take look to the embeddings of the word `king`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d0ff2a-4b21-4b1c-85ea-850858e63a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74501 , -0.11992 ,  0.37329 ,  0.36847 , -0.4472  , -0.2288  ,\n",
       "        0.70118 ,  0.82872 ,  0.39486 , -0.58347 ,  0.41488 ,  0.37074 ,\n",
       "       -3.6906  , -0.20101 ,  0.11472 , -0.34661 ,  0.36208 ,  0.095679,\n",
       "       -0.01765 ,  0.68498 , -0.049013,  0.54049 , -0.21005 , -0.65397 ,\n",
       "        0.64556 ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a2eaf-a043-4346-8761-792532b661c5",
   "metadata": {},
   "source": [
    "As you can see the token (word) king, is represented as a dense vector of type `float32`. With this representation, we can now compute the similarity between different embeddings, the following code snippet calculates the cosine similarity between `king` and `man` embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d001c3fe-add3-4564-9371-08fb76287a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666067"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(man, king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde7761c-8959-458e-a61f-f40959313042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310066"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(queen + man - woman , king)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f97dbe-5566-4a80-a732-324f8c9d0426",
   "metadata": {},
   "source": [
    "### 2.3 From Tokenization to Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a663e-ee53-419e-8fbb-f82f2574cbfd",
   "metadata": {},
   "source": [
    "As seen, in the section about tokenization, it is a process of breaking sequences into small pieces called tokens. As we covered many of the issues with LLMs come from bad tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162e56e-205b-4c5a-bec6-ad3efb924252",
   "metadata": {},
   "source": [
    "Now, after breaking the sequences into a list of tokens, we cannot directly plug them into a model as they are, because models understand only numbers, and vectors, i.e. **Embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951d184-e536-4d6d-a163-189d2a5c4190",
   "metadata": {},
   "source": [
    "So, the process is too simple, we map each token with embedding representations, that capture the semantic meaning of that specific token. These Embeddings are randomly initialized in the first place but will be learned during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771ccaa-09a4-40b7-ab0c-18019eb54611",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/tik_to_emb.png\" width=\"400\" />  \n",
    "  <figcaption> <b>From Tokenization to Embeddings</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333fefd2-711f-492f-b031-cfdcabb3243d",
   "metadata": {},
   "source": [
    "At last, we have arrived at this place, to talk about **Transformers**, I think we have covered all we need to start this topic. So, in the next sections, we're trying to explain all the building blocks of the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585dd42-d5c2-47a0-9836-86527d5f2f21",
   "metadata": {},
   "source": [
    "### 2.4 Understanding Weighted Aggregation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa96853-bae5-4557-b1ee-a9a2e7021061",
   "metadata": {},
   "source": [
    "**Weighted Aggregation** is a key concept to fully understand the **Attention Mechanism** in this section. we're going to cover an inefficient and efficient way to perform aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bd7a31-7cb8-431d-ae8e-9d56dd63c362",
   "metadata": {},
   "source": [
    "#### What's Aggregation in Machine Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d43b9-b34b-461a-93ca-034028dd1f41",
   "metadata": {},
   "source": [
    "**Aggregation** can be defined as the process of collecting data and information from different sources and presenting them in summarized form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d9a8b-d510-4faf-8db0-ff2ead032038",
   "metadata": {},
   "source": [
    "This concept can be found in many machine learning algorithms. For instance, consider a single neuron in a multi-layer perceptron (MLP). The output of the neuron is computed as a weighted sum of the input vectors, which is a form of aggregation:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i} w_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0c385-c774-4e94-9a56-7dcbe3b7867a",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) aggregate information by applying a kernel to an image, combining pixel values from the neighborhood to the center pixel. Similarly, Graph Neural Networks (GNNs) aggregate information through message passing, combining data from neighboring nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4080259-922e-4358-bbcc-94f6d7a7231e",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/conv_opt.gif\" width=\"400\" />  \n",
    "  <figcaption> <b>Convolution operation</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84c4fc-2a81-4c1e-a94a-1dcabce910b4",
   "metadata": {},
   "source": [
    "#### Weighted Aggregation (an Inefficient version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b4053-7b44-4c36-bc57-292068673c03",
   "metadata": {},
   "source": [
    "Before tackling the **Weighted Aggregation**, we're going step by step to introduce the concept while considering simple examples at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296e1f9-730e-41c0-8d5b-89e5f7c012b7",
   "metadata": {},
   "source": [
    "For instance, consider a list `a` with three values, we want at each item in the array `a`. We sum up all items from the beginning to the current item, then return the result as a new array. The computations are illustrated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_0 &= a_0 = 1 \\\\\n",
    "c_1 &= a_0 + a_1 = 1 + 2 = 3 \\\\\n",
    "c_2 &= a_0 + a_1 + a_2 = 1 + 2 + 3 = 6\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9701e-3fb0-4d35-9fbe-dd04b089263c",
   "metadata": {},
   "source": [
    "Beautiful, let's see this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d10449-83fe-4198-8d77-844e327c5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58147a6d-fc06-46af-b268-16cb2d5f30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e725cf-d69e-498b-a86e-c0798a1a3cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([np.sum(a[:i+1]) for i in range(len(a))])\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bdd65e-492c-4a53-bffc-213bf3e095d0",
   "metadata": {},
   "source": [
    "In the previous example, we used sum aggregation. But what if we want to use a different form of aggregation, such as **average**? We can achieve this by dividing the sum of the elements by the number of elements.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "c_0 &= \\frac{a_0}{1} = \\frac{1}{1} = 1 \\\\\n",
    "c_1 &= \\frac{a_0 + a_1}{2} = \\frac{1 + 2}{2} = \\frac{3}{2} = 1.5 \\\\\n",
    "c_2 &= \\frac{a_0 + a_1 + a_2}{3} = \\frac{1 + 2 + 3}{3} = \\frac{6}{3} = 2\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e159201-b7e9-4482-aa3f-83072cdbaf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.5, 2. ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([sum(a[:i+1])/(i+1) for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8071ae-a54b-40af-8ae0-060568136a1d",
   "metadata": {},
   "source": [
    "In fact, in the **Average Aggregation**, we multiply each element with a fix **weight** $w = \\frac{1}{\\text{num elements}}$, here where the **Weighted Aggregation** comes, at each iteration $i$ it performs aggregation of element $j$ with $w_{ij}$, and we have \n",
    "\n",
    "$$ \\forall i \\hspace{0.2cm} \\sum_{j}^{}w_{ij} = 1$$\n",
    "\n",
    "$$ \\forall i \\hspace{0.2cm} c_i = \\sum_{j}^{} w_{ij} a_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac5735-cfbb-4050-affa-f4daf70edd3e",
   "metadata": {},
   "source": [
    "Now, let's see all this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28094871-0411-4f51-bc59-3e450f79c71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc51852-81da-4dfe-ac08-71827c671a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [0.5, 0.5], [0.1, 0.4, 0.5]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [[1], [0.5, 0.5], [0.1, 0.4, 0.5]]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f8ec34-fb68-4246-9c50-199817369e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1.5, 2.4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [ sum(w[i][j] * a[j] for j in range(len(a[:i+1]))) for i in range(len(a))]\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9f135-0b89-43aa-8678-c44562296d46",
   "metadata": {},
   "source": [
    "<div class=\"admonition question\" markdown>\n",
    "    <p class=\"admonition-title\">Question</p>\n",
    "    <p>\n",
    "    You may now ask, <b>Why and When we will need this?</b>   \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd4cd3-0722-45b8-a036-5a25150f411a",
   "metadata": {},
   "source": [
    "Great question, the answer is that we're building the concepts that we'll need to understand the **Attention Mechanism** so make sure that you understand these key ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acfa6a-2908-45ae-9b41-2a4ee632e7e6",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\" markdown>\n",
    "    <p class=\"admonition-title\">Keep In Mind</p>\n",
    "    <p>The most important thing to keep in mind. <b> Weighted Aggregation </b> is a way to aggregate previous information, but in a weighted way which means we define how much each element can contribute. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a747cc6d-8b4d-4629-99e0-a6bfb6dffdc9",
   "metadata": {},
   "source": [
    "#### Weighted Aggregation (An effecient Way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54abf3-00e2-45eb-b838-75fab44a2c5e",
   "metadata": {},
   "source": [
    "In the previous section we performed, weighted aggregation using for loops which isn't efficient. There is a better way to turn it into matrix multiplication?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b77356-960a-4563-870d-8670f4149506",
   "metadata": {},
   "source": [
    "Hey, what just turning it into matrix multiplication makes this operation efficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19630c-9923-4521-a22a-ec6b8cbdb2fe",
   "metadata": {},
   "source": [
    "The short answer is yes! I'll explain. Computers excel at performing linear algebra. **GPU** hardware is specifically optimized to handle matrix multiplication and linear algebra operations with high efficiency.\n",
    "\n",
    "For more information, check out:\n",
    "\n",
    "- **Why linear algebra and GPUs are the bedrock of machine learning** [https://b-yarbrough.medium.com/why-linear-algebra-and-gpus-are-the-bedrock-of-machine-learning-4a55baac2897](https://b-yarbrough.medium.com/why-linear-algebra-and-gpus-are-the-bedrock-of-machine-learning-4a55baac2897)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38199c59-9f2f-464e-8ef7-aeb92eb7dfb1",
   "metadata": {},
   "source": [
    "Now, let's back to our main topic here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d6003-83ca-481f-926f-17389811725c",
   "metadata": {},
   "source": [
    "The key idea is to construct the `w` as the upper triangular portion as shown in the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "939a2e14-32e8-42a9-8090-1473be9849e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. , 0. ],\n",
       "       [0.5, 0.5, 0. ],\n",
       "       [0.1, 0.4, 0.5]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([[1  ,   0,   0],\n",
    "              [0.5, 0.5,   0],\n",
    "              [0.1, 0.4, 0.5]])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd83d4-493f-4892-91aa-f9035fbee778",
   "metadata": {},
   "source": [
    "Then multiply this matrix with the `a` array, which we have before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a317c40-668f-4eab-b6c3-1fc26eef4608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea61075-9d24-46c4-b9bb-a507d9a530d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.5, 2.4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w @ a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c0557-5f1a-4721-8ac7-cff456213252",
   "metadata": {},
   "source": [
    "WooW, As we can see. We've just done the same thing as we did before. It is easy to get, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e67293-8b2e-4c28-b74f-45ddd2cc2a2b",
   "metadata": {},
   "source": [
    "The last thing I want to explain in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aea704-cca4-41c0-8b57-d617e46a7524",
   "metadata": {},
   "source": [
    "What if `a` is not a vector but a matrix, the aggregation will be performed with respect to rows. This can be done by changing only the value of `a`, let's have a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd17724c-72fb-4ec8-a008-0fb38ba98e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 4],\n",
    "              [2, 5],\n",
    "              [3, 6]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6c4df60-9c0e-4778-a2be-bc350cc8fe75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [0.5       , 0.5       , 0.        ],\n",
       "       [0.33333333, 0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define out\n",
    "w = np.array([[1  ,   0,   0],\n",
    "              [1/2, 1/2,   0],\n",
    "              [1/3, 1/3, 1/3]])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c39b9166-7f0a-4d26-b025-bc0a7dc1d3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 4. ],\n",
       "       [1.5, 4.5],\n",
       "       [2. , 5. ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w @ a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffcc20-385e-40e5-b9f2-0922e7d7e922",
   "metadata": {},
   "source": [
    "<!-- ### Extra-curriculum\n",
    "- **Attention, Transformers, and GPT** (Medium Blog) [https://trevormcguire.medium.com/attention-transformers-and-gpt-b3adbbb4a950](https://trevormcguire.medium.com/attention-transformers-and-gpt-b3adbbb4a950) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6510d0-72d1-40d3-a89f-4d4c75f8d046",
   "metadata": {},
   "source": [
    "Until now, we've covered many basic concepts but they are essential! such as **Tokenization**, **Embeddings**, **Weighted Aggregation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70081502-d5eb-4acf-bbc2-b845960f3dfd",
   "metadata": {},
   "source": [
    "## 03. Transformer's Embedding Layer and Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb6c14-b052-42af-b0ae-d1afe3cce4ce",
   "metadata": {},
   "source": [
    "Now, it's time to start tackling the **Transformer** architecture, in the following section we're going start learning about Transformer's **Embedding layer**, as well as the **Positional Encoding**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f300ee-6a0f-4a87-aede-e894d175897f",
   "metadata": {},
   "source": [
    "<figure markdown=\"span\">\n",
    "    <center>\n",
    "  <img src=\"https://raw.githubusercontent.com/jamormoussa/docs/main/docs/images/llms/emb_layer.png\" width=\"400\" />  \n",
    "  <figcaption> <b>Embedding Layer & Positional Encoding</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418003a4-2738-488f-9aa8-cf85232d02ad",
   "metadata": {},
   "source": [
    "### 3.1 Embedding Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa303d-5ba6-4d25-925e-d68aaafce3f0",
   "metadata": {},
   "source": [
    "**Embedding Layer** is a looking table of dimensions **vocabulary size** and **Embedding Dimension**, the embeddings dimension denoted in the original paper as $d_{model} = 512$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c8cf7-420c-480d-9696-a410499d0fcd",
   "metadata": {},
   "source": [
    "This Embedding layer accepts as input a tenor of indexes (tokens) and returns the corresponding embedding vectors. `PyTorch` has a built-in embedding layer from `nn` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c902231b-ea2f-4784-8d8b-1a6d2d9b65f0",
   "metadata": {},
   "source": [
    "First, we import the `torch` as `torch.nn` module, and `tiktoken` library to use `gpt2` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "58d96510-04a5-494c-9e9b-3c055d722846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76f12c-e98e-426f-adf7-e70ba20e6ae6",
   "metadata": {},
   "source": [
    "Let's get the `gpt2` tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7568d64-2207-4fe0-aa24-0b47bda1b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e0f01-e9d3-43dd-bea5-f3af180f5dcf",
   "metadata": {},
   "source": [
    "the `gpt2` tokenizer has `50257` in the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae6b704e-7507-44f2-b296-e107b3efaaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855fc5a-e56d-4650-8e5c-91e2ba66e136",
   "metadata": {},
   "source": [
    "Now, let's create an instance of the embeddings layer from the `nn` module, with specifying two arguments.\n",
    "\n",
    "- `num_embeddings`: represents the vocabulary size, we can set its value equal to `tokenizer.n_vocab`.\n",
    "  \n",
    "- `embeddings_dim`: represents how many dimensions you want to use to represent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afe0f53d-6e2f-4d37-a5a8-a2f196cd60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = nn.Embedding(\n",
    "    num_embeddings= tokenizer.n_vocab, # voab_size of gpt2 tokenizer\n",
    "    embedding_dim= 512 # d_model: embeddings dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e655b-1565-40db-8cde-36f8f3d1bcaa",
   "metadata": {},
   "source": [
    "Let's consider that we have the following sentence `Hi my name is Moussa`, in which we encode using the `gpt2` tokenizer. then we get the following `sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41705a20-4b71-4350-9f8d-9ca5d789e124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17250, 616, 1438, 318, 42436, 11400]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = tokenizer.encode(\"Hi my name is Moussa\")\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5f1d94e-e044-4760-abc2-096c4a3383de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17250,   616,  1438,   318, 42436, 11400]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = torch.LongTensor([sentence]) # LongTensor to ensure that indexes are integers.\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e4f390b-6899-4b52-8367-398ac07e1bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59be3e-b5d9-43c5-878e-a317d04ddd35",
   "metadata": {},
   "source": [
    "The `sequence` tensor is of shape `(1, 6)`, the first dimension represents the **batch** dimension, here `batch = 1` because we have a single sentence. The second dimension is the length of your sequence, also called the **Time** dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2913c099-e2b0-4d0b-a017-7454a10a052b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8604,  0.2464,  2.5279,  ..., -1.5272,  1.4942,  0.6481],\n",
       "         [ 1.7703, -1.0316,  0.8046,  ...,  0.3494,  0.2895, -1.6490],\n",
       "         [-0.7849, -1.7873, -0.4447,  ...,  0.9301,  0.4982, -1.4281],\n",
       "         [-0.3122,  0.4844,  0.0659,  ...,  0.0458, -0.3955, -0.0980],\n",
       "         [-0.8068, -0.3864,  0.0838,  ..., -1.3389,  0.2242,  0.1750],\n",
       "         [-1.1558, -0.2892, -1.2714,  ..., -0.4282, -2.8925, -1.1820]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = emb_layer(sequence)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f4e80-253c-476b-b949-e943d37b086d",
   "metadata": {},
   "source": [
    "As you can see the embeddings layer has grad function`grad_fn=<EmbeddingBackward0>` which indicates that it is trainable. These embeddings will be changed and updated during the back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "babfdce8-f85a-464e-bfd8-7ec3a4a7e655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 512])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd3962-505a-4aa3-b6af-af538d1b9f6f",
   "metadata": {},
   "source": [
    "Let's debug the shape of embeddings that have the form of `(B, T, d_model)`.\n",
    "\n",
    "- **`B`**: represents the batch dimension. how many sentences or sequences do you want to feed the transformer? The batch dimension has a nice property. All sequences are independent of each other, which allows us to process them in parallel and benefit from the computer's power.\n",
    "  \n",
    "- **`T`**: is the Time dimension, represents the length of the sequence. how many tokens are in the sequence? The time in the context of using transformer's architecture (key characteristic) is all tokens are fed at the same time, hence the time dimension is considered as batch dimension.\n",
    "  \n",
    "- **`d_model`**: also called embeddings dimension, or number of channels. It represents how many dimensions you use to represent the tokens. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
