{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Zero to Mastery Learn Machine Learning and Deep Learning Concepts","text":"<p>Hi, I'm Moussa JAMOR \ud83d\ude01.</p> <p>A Machine Learning Engineer Student at ENSIAS \ud83d\udc68\u200d\ud83d\udcbb.</p> <p>This website Zero to Mastery Learn Machine Learning and Deep Learning Concepts will contains all concepts and ressource materials, that I learned during my learning journey in field of Artificial Intelligence, Machine Learning, Deep Learning and Data Science.</p>"},{"location":"#projects","title":"Projects","text":"<p>Currently, I'm working on some intersting projects:</p> <ul> <li> <p><code>NanoTorch</code>: is Deep Learning Library from scratch using Numpy and Math. </p> </li> <li> <p><code>torch_utils</code>: is a PyTorch extension designed for training and building deep learning models. </p> </li> </ul>"},{"location":"deep_learning/","title":"Deep Learning","text":""},{"location":"deep_learning/#hello","title":"Hello","text":"<p>$$     X^2 $$</p>"},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/","title":"AutoEncoders Architecture In DeepLearning","text":"<p>This Notebook is created by Moussa Jamor</p> <p>Notebook link (Github): AutoEncoders in Deep Learning</p> <p>The AutoEncoders are special type of neural networks used for unsupervised learning. They composed by two main components, the Encoder and the Decoder, which both are neural networks architecture. In this notebook, you will have everything need to know about AutoEncoders, including the theory as well as build a AutoEncoder model using PyTorch, the dataset we'll use is MNIST dataset. As well as, see What's some AutoEncoders's applications.</p> <p>First, the AutoEncoders proposed by G. E. Hinton and R. R. Salakhutdinov in paper titled Reducing the Dimensionality of Data with Neural Networks. They proposed the AutoEncoders as Non-Linear generatisation of PCA, dimentionality reduction cases. But, AutoEncoders has widely used in other applications, Transfer Leaning, Generative Models, Anomaly Detection and more.</p> <p>Let's build and train our first AutoEncoders model. So, this section we will define the model's architecture, then we'll use the well known MNIST dataset, it's available through the <code>pytorchvision</code> library, for further information see the torchvision MNIST documentation.</p> <p>Let's start by importing necessary libraries. Let's begin with <code>PyTorch</code> \ud83d\udd25.</p> In\u00a0[166]: Copied! <pre>import torch, torch.nn as nn\nimport torch.optim as optim \nfrom torch.utils.data import DataLoader, random_split, Subset\n\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\n\nfrom sklearn.metrics import classification_report\n</pre> import torch, torch.nn as nn import torch.optim as optim  from torch.utils.data import DataLoader, random_split, Subset  import torchvision import torchvision.datasets as datasets import torchvision.transforms as transforms  import matplotlib.pyplot as plt import numpy as np  import seaborn as sns  from sklearn.metrics import classification_report <p>In this section, we define some helping function, that we're going to use in coming sections.</p> In\u00a0[4]: Copied! <pre># define function to plot handwrite digits images \n\ndef imshow_image(tensor: torch.Tensor) -&gt; None:\n    plt.imshow(tensor.detach().view(28, 28).numpy())\n</pre> # define function to plot handwrite digits images   def imshow_image(tensor: torch.Tensor) -&gt; None:     plt.imshow(tensor.detach().view(28, 28).numpy()) In\u00a0[5]: Copied! <pre>def plot_subplots(\n    images: torch.Tensor,\n) -&gt; None:\n\n    max_cols = 8\n\n    fig, axes = plt.subplots(1, max_cols, figsize=(18, 2))\n\n    num_cols = images.shape[0]\n\n    img = images.detach().numpy()\n\n    for col in range(num_cols):\n        if col &gt;= max_cols: break\n        axes[col].imshow(img[1 + col])\n    plt.show()\n</pre> def plot_subplots(     images: torch.Tensor, ) -&gt; None:      max_cols = 8      fig, axes = plt.subplots(1, max_cols, figsize=(18, 2))      num_cols = images.shape[0]      img = images.detach().numpy()      for col in range(num_cols):         if col &gt;= max_cols: break         axes[col].imshow(img[1 + col])     plt.show() <p>To download and load the MNSIT dataset, we use the built-in <code>MNSIT</code> class from <code>torchvision.datasets</code>. Then sepecify some transformations by converting images to Tensor, using <code>transforms.ToTensor()</code>, each image is matrix of 28x28, the <code>nn.Flatten</code> transform this matrix to vector with 728 items.</p> In\u00a0[6]: Copied! <pre>!mkdir mnist\n</pre> !mkdir mnist <pre>mkdir: cannot create directory \u2018mnist\u2019: File exists\n</pre> In\u00a0[7]: Copied! <pre>mnist = datasets.MNIST('./mnist/', download=True, \n                      transform= transforms.Compose([\n                          transforms.ToTensor(),\n                          nn.Flatten()\n                      ])\n        )\n</pre> mnist = datasets.MNIST('./mnist/', download=True,                        transform= transforms.Compose([                           transforms.ToTensor(),                           nn.Flatten()                       ])         ) <p>After the MNSIT dataset is downloaded and loaded successflly, we can viusalize using one of helping functions <code>imshow_image</code>, which we define at the beginning.</p> In\u00a0[8]: Copied! <pre>imshow_image(mnist[0][0])\n</pre> imshow_image(mnist[0][0]) <p>The sections we split the MNIST dataset, into a train with 50000 images, and test with 10000 images.</p> In\u00a0[9]: Copied! <pre>train_data, test_data = random_split(mnist, [50000, 10000])\n</pre> train_data, test_data = random_split(mnist, [50000, 10000]) <p>To train our model, we have to create a DataLoader for each dataset train/test. with <code>batch_size = 25</code> for training. For test dataset, we use the whole dataset.</p> In\u00a0[10]: Copied! <pre>batch_size = 25\n\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=10000, shuffle=True)\n</pre> batch_size = 25   train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True) test_loader = DataLoader(test_data, batch_size=10000, shuffle=True) In\u00a0[11]: Copied! <pre>input, _ = next(iter(train_loader))\n</pre> input, _ = next(iter(train_loader)) <p>After creating the DataLoader, let's plot the 8 first hand-writing digits, using the <code>plot_subplots</code> function.</p> In\u00a0[12]: Copied! <pre>plot_subplots(input.squeeze(1).view(-1, 28, 28))\n</pre> plot_subplots(input.squeeze(1).view(-1, 28, 28)) <p>This part is most exciting section, we're going to build our first AutoEncoder Model with PyTorch \ud83d\udd25. As explained in the previous parts, That the AutoEncoders have two main components and building blocks. which are the Encoder and the Decoder component.</p> <p>Note:</p> <ul> <li>There is no standard way to build a create a AutoEncoders architecture. Which means, we can use Vanilla MLP, ConvLayers, RNN ..., etc.</li> <li>in our case we'll use CNN, because it performs better for images.</li> </ul> <p>Let's start with the Encoder. Are exciting, offcourse you are \ud83d\ude0a.</p> <p>Finally, we create a <code>Encoder</code> class, that extends for <code>nn.Module</code>, we create two main sub-models. The <code>conv_encoder</code> which is features extractor. Then, we fit the result to a <code>linear_encoder</code> that return the Embeddings representation of the input. At the first place, the model is initilized randomly but, it will be better and better during the training phase.</p> In\u00a0[13]: Copied! <pre>class Encoder(nn.Module):\n\n    def __init__(self, input_size: int = 28*28, embeddings_size = 10) -&gt; None:\n        super(Encoder, self).__init__()\n\n        self.conv_encoder = nn.Sequential(\n            nn.Conv2d(1, 10, 3),\n            nn.Conv2d(10, 8, 3),\n            nn.ReLU(),\n            nn.Conv2d(8, 4, 7),\n            nn.Flatten(1, -1),\n        )\n\n        self.linear_encoder = nn.Sequential(\n            nn.Linear(4 * 18 * 18, 128),\n            nn.Linear(128, 32),\n            nn.ReLU(),\n            nn.Linear(32, embeddings_size)\n        )\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        out = input.view(-1, 1, 28, 28)\n        out = self.conv_encoder(out)\n        return self.linear_encoder(out)\n</pre> class Encoder(nn.Module):      def __init__(self, input_size: int = 28*28, embeddings_size = 10) -&gt; None:         super(Encoder, self).__init__()          self.conv_encoder = nn.Sequential(             nn.Conv2d(1, 10, 3),             nn.Conv2d(10, 8, 3),             nn.ReLU(),             nn.Conv2d(8, 4, 7),             nn.Flatten(1, -1),         )          self.linear_encoder = nn.Sequential(             nn.Linear(4 * 18 * 18, 128),             nn.Linear(128, 32),             nn.ReLU(),             nn.Linear(32, embeddings_size)         )      def forward(self, input: torch.Tensor) -&gt; torch.Tensor:         out = input.view(-1, 1, 28, 28)         out = self.conv_encoder(out)         return self.linear_encoder(out) <p>Let's create an instance of <code>Encoder</code> class, to see how work if we fit it, with an images as input.</p> In\u00a0[14]: Copied! <pre>encoder = Encoder()\n</pre> encoder = Encoder() In\u00a0[15]: Copied! <pre>input, _ = next(iter(train_loader))\n</pre> input, _ = next(iter(train_loader)) <p>The following code give the dense vector representation, for first images. Let's create the Decoder class.</p> In\u00a0[16]: Copied! <pre>encoder(input)[0]\n</pre> encoder(input)[0] Out[16]: <pre>tensor([ 0.0258,  0.1940, -0.1093,  0.0069,  0.0537,  0.1174, -0.0988,  0.1132,\n         0.1323,  0.1360], grad_fn=&lt;SelectBackward0&gt;)</pre> <p>The <code>Decoder</code> class is created by extendings as well from <code>nn.Module</code>. the Decoder is simple the inverse architecture of the Encoder. For Convolution part, we use the <code>nn.ConvTranspose2d</code> to perform the inverse operation of <code>nn.Conv2d</code>.</p> In\u00a0[17]: Copied! <pre>class Decoder(nn.Module):\n    def __init__(self, input_size: int = 10, embeddings_size=10):\n        super(Decoder, self).__init__()\n        \n        self.linear_decoder = nn.Sequential(\n            nn.Linear(embeddings_size, 32),\n            nn.ReLU(),\n            nn.Linear(32, 128),\n            nn.ReLU(),\n            nn.Linear(128, 4 * 18 * 18),\n            nn.Unflatten(1, (4, 18, 18))\n        )\n\n        self.conv_decoder = nn.Sequential(\n            nn.ConvTranspose2d(4, 8, 7),\n            nn.ReLU(),\n            nn.ConvTranspose2d(8, 10, 3),\n            nn.ReLU(),\n            nn.ConvTranspose2d(10, 1, 3),\n        )\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        out = self.linear_decoder(input)\n        return self.conv_decoder(out.view(-1, 4, 18, 18))\n</pre> class Decoder(nn.Module):     def __init__(self, input_size: int = 10, embeddings_size=10):         super(Decoder, self).__init__()                  self.linear_decoder = nn.Sequential(             nn.Linear(embeddings_size, 32),             nn.ReLU(),             nn.Linear(32, 128),             nn.ReLU(),             nn.Linear(128, 4 * 18 * 18),             nn.Unflatten(1, (4, 18, 18))         )          self.conv_decoder = nn.Sequential(             nn.ConvTranspose2d(4, 8, 7),             nn.ReLU(),             nn.ConvTranspose2d(8, 10, 3),             nn.ReLU(),             nn.ConvTranspose2d(10, 1, 3),         )      def forward(self, input: torch.Tensor) -&gt; torch.Tensor:         out = self.linear_decoder(input)         return self.conv_decoder(out.view(-1, 4, 18, 18))  <p>Let's create a instance of <code>Decoder</code> class, then fit the model with random vector to construct an image.</p> In\u00a0[18]: Copied! <pre>decoder = Decoder()\n</pre> decoder = Decoder() In\u00a0[19]: Copied! <pre>img = decoder(torch.randn(1, 10))\n</pre> img = decoder(torch.randn(1, 10)) In\u00a0[20]: Copied! <pre>imshow_image(img)\n</pre> imshow_image(img) <p>To create a <code>AutoEncoders</code> model, we compose the <code>Encoder</code> and the <code>Decoder</code> class.</p> <p>Finally, we dit it.</p> In\u00a0[21]: Copied! <pre>class AutoEncoders(nn.Module):\n\n    def __init__(self, input_size=28*28, embeddings_size=100) -&gt; None:\n        super(AutoEncoders, self).__init__()\n\n        self.encoder = Encoder(input_size, embeddings_size)\n        self.decoder = Decoder(embeddings_size=embeddings_size)\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        out = self.encoder(input)\n        return self.decoder(out)\n</pre> class AutoEncoders(nn.Module):      def __init__(self, input_size=28*28, embeddings_size=100) -&gt; None:         super(AutoEncoders, self).__init__()          self.encoder = Encoder(input_size, embeddings_size)         self.decoder = Decoder(embeddings_size=embeddings_size)      def forward(self, input: torch.Tensor) -&gt; torch.Tensor:         out = self.encoder(input)         return self.decoder(out) <p>Let's create a instance of <code>AutoEncoders</code> model, and see the whole architecture.</p> In\u00a0[22]: Copied! <pre>model = AutoEncoders()\n</pre> model = AutoEncoders() In\u00a0[23]: Copied! <pre>model\n</pre> model Out[23]: <pre>AutoEncoders(\n  (encoder): Encoder(\n    (conv_encoder): Sequential(\n      (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n      (1): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1))\n      (2): ReLU()\n      (3): Conv2d(8, 4, kernel_size=(7, 7), stride=(1, 1))\n      (4): Flatten(start_dim=1, end_dim=-1)\n    )\n    (linear_encoder): Sequential(\n      (0): Linear(in_features=1296, out_features=128, bias=True)\n      (1): Linear(in_features=128, out_features=32, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=32, out_features=100, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (linear_decoder): Sequential(\n      (0): Linear(in_features=100, out_features=32, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=32, out_features=128, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=128, out_features=1296, bias=True)\n      (5): Unflatten(dim=1, unflattened_size=(4, 18, 18))\n    )\n    (conv_decoder): Sequential(\n      (0): ConvTranspose2d(4, 8, kernel_size=(7, 7), stride=(1, 1))\n      (1): ReLU()\n      (2): ConvTranspose2d(8, 10, kernel_size=(3, 3), stride=(1, 1))\n      (3): ReLU()\n      (4): ConvTranspose2d(10, 1, kernel_size=(3, 3), stride=(1, 1))\n    )\n  )\n)</pre> In\u00a0[24]: Copied! <pre>img = model(mnist[0][0])\n</pre> img = model(mnist[0][0]) In\u00a0[25]: Copied! <pre>img.shape\n</pre> img.shape Out[25]: <pre>torch.Size([1, 1, 28, 28])</pre> In\u00a0[26]: Copied! <pre>imshow_image(img)\n</pre> imshow_image(img) <p>Now, it's the time for trainnig.</p> <p>Since, the AutoEncoders model, try to constuct the fitted image as input. So, let's consider as regression problem. that the model try to predict each pixel. Thus, <code>nn.MSELoss</code> is used as loss function to the model.</p> In\u00a0[27]: Copied! <pre>criterion = nn.MSELoss()\n</pre> criterion = nn.MSELoss() In\u00a0[28]: Copied! <pre># define model parameters to be updated during the back-probagation \n\nparams_to_optimize = [\n    {'params': model.encoder.parameters()},\n    {'params': model.decoder.parameters()}\n]\n</pre> # define model parameters to be updated during the back-probagation   params_to_optimize = [     {'params': model.encoder.parameters()},     {'params': model.decoder.parameters()} ] <p>A <code>optim.Adam</code> optimizer used with a learning rate of 0.001.</p> In\u00a0[29]: Copied! <pre>opt = optim.Adam(params_to_optimize,lr=0.001)\n</pre> opt = optim.Adam(params_to_optimize,lr=0.001) In\u00a0[30]: Copied! <pre># This function is define to train the model, with MSELoss, and Adam optimizer.\n\ndef train(\n    model,\n    criterion,\n    optimizer,\n    train_loader,\n    epochs=1,\n    loggings: bool = True, \n    loggings_iter: int = 400,\n) -&gt; None:\n    \n    model.train()\n    \n    for epoch in range(epochs):\n        \n        for i, (img, _) in enumerate(train_loader):\n        \n            optimizer.zero_grad()\n        \n            gen_img = model(img)\n            \n            loss = criterion(gen_img.flatten(2,-1), img)\n            \n            loss.backward()\n            \n            optimizer.step()\n\n            if i%int(loggings_iter)==0:\n                print(f\"Epochs: {epoch:4d} | Iteration: {i:4d}| Loss: {loss.item():4.7f}\")\n                print(\"-\"*140)\n                plot_subplots(img.squeeze(1).view(-1, 28, 28))\n                plot_subplots(gen_img.squeeze(1).view(-1, 28, 28))\n\n    print(\"-\"*140)\n    print(\"Traning is finished :) \")\n</pre> # This function is define to train the model, with MSELoss, and Adam optimizer.  def train(     model,     criterion,     optimizer,     train_loader,     epochs=1,     loggings: bool = True,      loggings_iter: int = 400, ) -&gt; None:          model.train()          for epoch in range(epochs):                  for i, (img, _) in enumerate(train_loader):                      optimizer.zero_grad()                      gen_img = model(img)                          loss = criterion(gen_img.flatten(2,-1), img)                          loss.backward()                          optimizer.step()              if i%int(loggings_iter)==0:                 print(f\"Epochs: {epoch:4d} | Iteration: {i:4d}| Loss: {loss.item():4.7f}\")                 print(\"-\"*140)                 plot_subplots(img.squeeze(1).view(-1, 28, 28))                 plot_subplots(gen_img.squeeze(1).view(-1, 28, 28))      print(\"-\"*140)     print(\"Traning is finished :) \") In\u00a0[46]: Copied! <pre># Let's start trainig, enjoy the show :).\n\ntrain(\n    model, \n    criterion, \n    optimizer=opt,\n    train_loader=train_loader,\n    epochs=2\n)\n</pre> # Let's start trainig, enjoy the show :).  train(     model,      criterion,      optimizer=opt,     train_loader=train_loader,     epochs=2 ) <pre>Epochs:    0 | Iteration:    0| Loss: 0.1339402\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    0 | Iteration:  400| Loss: 0.0267787\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    0 | Iteration:  800| Loss: 0.0217806\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    0 | Iteration: 1200| Loss: 0.0183262\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    0 | Iteration: 1600| Loss: 0.0168445\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    1 | Iteration:    0| Loss: 0.0182455\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    1 | Iteration:  400| Loss: 0.0134456\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    1 | Iteration:  800| Loss: 0.0134334\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    1 | Iteration: 1200| Loss: 0.0128348\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>Epochs:    1 | Iteration: 1600| Loss: 0.0105310\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> <pre>--------------------------------------------------------------------------------------------------------------------------------------------\nTraning is finished :) \n</pre> <p>The trainnig is finished, very nice. We can notice that model starts with noise images, but only after 400 iterations, we already indentify digits.</p> <p>After the model trained, it's best practice to save out model, for safety reasons, ex. avoiding kernel restart in jupyter notebook and loss the model, let's save the whole model, then each components alone.</p> <p>Why? the reason, is after training, we can split the encoder from the decoder parts and each components can be used in a different task. see the Applications section for further information.</p> In\u00a0[31]: Copied! <pre># torch.save(model.state_dict(), \"./models/AutoEncoders_V1.pth\")\n</pre> # torch.save(model.state_dict(), \"./models/AutoEncoders_V1.pth\") In\u00a0[32]: Copied! <pre># torch.save(model.encoder.state_dict(), \"./models/Encoder_V1.pth\")\n</pre> # torch.save(model.encoder.state_dict(), \"./models/Encoder_V1.pth\") In\u00a0[33]: Copied! <pre># torch.save(model.decoder.state_dict(), \"./models/Decoder_V1.pth\")\n</pre> # torch.save(model.decoder.state_dict(), \"./models/Decoder_V1.pth\") In\u00a0[34]: Copied! <pre>model = AutoEncoders()\n</pre> model = AutoEncoders() In\u00a0[35]: Copied! <pre>model.load_state_dict(torch.load(\"./models/AutoEncoders_V1.pth\"))\n</pre> model.load_state_dict(torch.load(\"./models/AutoEncoders_V1.pth\")) Out[35]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[36]: Copied! <pre>model\n</pre> model Out[36]: <pre>AutoEncoders(\n  (encoder): Encoder(\n    (conv_encoder): Sequential(\n      (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n      (1): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1))\n      (2): ReLU()\n      (3): Conv2d(8, 4, kernel_size=(7, 7), stride=(1, 1))\n      (4): Flatten(start_dim=1, end_dim=-1)\n    )\n    (linear_encoder): Sequential(\n      (0): Linear(in_features=1296, out_features=128, bias=True)\n      (1): Linear(in_features=128, out_features=32, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=32, out_features=100, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (linear_decoder): Sequential(\n      (0): Linear(in_features=100, out_features=32, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=32, out_features=128, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=128, out_features=1296, bias=True)\n      (5): Unflatten(dim=1, unflattened_size=(4, 18, 18))\n    )\n    (conv_decoder): Sequential(\n      (0): ConvTranspose2d(4, 8, kernel_size=(7, 7), stride=(1, 1))\n      (1): ReLU()\n      (2): ConvTranspose2d(8, 10, kernel_size=(3, 3), stride=(1, 1))\n      (3): ReLU()\n      (4): ConvTranspose2d(10, 1, kernel_size=(3, 3), stride=(1, 1))\n    )\n  )\n)</pre> <p>In fact, After the AutoEncoders model is trained. We can use the Encoder network to get a vectors representation of a given input, the Decoder try to construct the data, the following represents some use case of AutoEncoders:</p> <ul> <li><p>Dimensionality Reduction: see the following section, the AutoEncoders are considered as Non-Linear generalisation of PCA.</p> </li> <li><p>Classification/Regression Cases: The Encoder is also considered as feature-extractor, means during the back-propagation the most important features/patterns. So, we can use it as Transfer Learning, which means we can connect to a classifier or regressor to perform a Reression or classification for specific problem.</p> </li> <li><p>Compute Similarity: We can fit two samples, to the Encoder and get their representations, which are vectors, then compute a Cosine metric to measure similarity between these inputs.</p> </li> <li><p>Generating new instances/ Data Augmentation: After the model is trained, we can use Decoder to generate new instances for us, a use case if you have not must data using this techniques, is powerful becuase it learn the distribution during the training.</p> </li> </ul> <p>The AutoEncoder arhitecture was first proposed as Non-Linear generatisation of PCA in the paper, titled Reducing the Dimensionality of Data with Neural Networks. As we see in previous sections, that AutoEncoders comes with two networks, the Encoder and the Decoder network. The Encoder tends to learn the features and patters from input data, it try to transform the hight-dimension data to low-dimentional space called Embeddings denoted by $Z$. In other hand, the Decoder network, tends to restruct the data given the Embeddings $Z$.</p> <p>In the article An Introduction to Autoencoders written by Umberto Michelucci, in the dimensionality reduction section, he compares autoencoders with PCA:</p> <ul> <li><p>Autoencoders can deal with a very large amount of data very efficiently since their training can be done with mini-batches, while PCA operates on the entire dataset. This can be an issue when the dataset is very large.</p> </li> <li><p>PCA provides a linear transformation, which may not capture non-linear relationships well. However, autoencoders are more flexible; by using activation functions, we can capture these kinds of information.</p> </li> </ul> In\u00a0[178]: Copied! <pre>class MnistClassifier(nn.Module):\n\n    def __init__(self) -&gt; None:\n        super(MnistClassifier, self).__init__()\n\n        _base_model = AutoEncoders()\n        _base_model.load_state_dict(torch.load(\"./models/AutoEncoders_V1.pth\"))\n\n        self.encoder = _base_model.encoder\n\n        self.classifier = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.ReLU(),\n            nn.Linear(32, 10)\n        )\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        out = self.encoder(input)\n        return self.classifier(out)\n</pre> class MnistClassifier(nn.Module):      def __init__(self) -&gt; None:         super(MnistClassifier, self).__init__()          _base_model = AutoEncoders()         _base_model.load_state_dict(torch.load(\"./models/AutoEncoders_V1.pth\"))          self.encoder = _base_model.encoder          self.classifier = nn.Sequential(             nn.Linear(100, 32),             nn.ReLU(),             nn.Linear(32, 10)         )      def forward(self, input: torch.Tensor) -&gt; torch.Tensor:         out = self.encoder(input)         return self.classifier(out)      In\u00a0[179]: Copied! <pre>clf = MnistClassifier()\n</pre> clf = MnistClassifier() In\u00a0[180]: Copied! <pre>clf\n</pre> clf Out[180]: <pre>MnistClassifier(\n  (encoder): Encoder(\n    (conv_encoder): Sequential(\n      (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n      (1): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1))\n      (2): ReLU()\n      (3): Conv2d(8, 4, kernel_size=(7, 7), stride=(1, 1))\n      (4): Flatten(start_dim=1, end_dim=-1)\n    )\n    (linear_encoder): Sequential(\n      (0): Linear(in_features=1296, out_features=128, bias=True)\n      (1): Linear(in_features=128, out_features=32, bias=True)\n      (2): ReLU()\n      (3): Linear(in_features=32, out_features=100, bias=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=100, out_features=32, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=32, out_features=10, bias=True)\n  )\n)</pre> In\u00a0[183]: Copied! <pre>imgs, labels = next(iter(train_loader))\n</pre> imgs, labels = next(iter(train_loader)) In\u00a0[184]: Copied! <pre>imshow_image(imgs[0])\n</pre> imshow_image(imgs[0]) In\u00a0[185]: Copied! <pre>clf(imgs[0])\n</pre> clf(imgs[0]) Out[185]: <pre>tensor([[-0.0270, -0.0376, -0.2098,  0.0372, -0.0946,  0.6591, -0.2061, -0.0600,\n         -0.0110, -0.1383]], grad_fn=&lt;AddmmBackward0&gt;)</pre> In\u00a0[186]: Copied! <pre>criterion = nn.CrossEntropyLoss()\n</pre> criterion = nn.CrossEntropyLoss() In\u00a0[187]: Copied! <pre>opt = optim.Adam(clf.classifier.parameters(), lr=0.001)\n</pre> opt = optim.Adam(clf.classifier.parameters(), lr=0.001) In\u00a0[191]: Copied! <pre>def train(epochs: int = 1): \n\n    for epoch in range(epochs):\n        \n        for i, (imgs, labels) in enumerate(train_loader):\n        \n            opt.zero_grad()\n        \n            labels_pred = clf(imgs)\n        \n            loss = criterion(labels_pred, labels.clone().detach())\n            \n            loss.backward()\n            \n            opt.step()\n    \n            if i%500==0 or i==0:\n                print(f\"Epochs: {epoch:4d} | Iteration: {i:4d}| Loss: {loss.item():4.7f}\")\n                print(\"-\"*140)\n</pre> def train(epochs: int = 1):       for epoch in range(epochs):                  for i, (imgs, labels) in enumerate(train_loader):                      opt.zero_grad()                      labels_pred = clf(imgs)                      loss = criterion(labels_pred, labels.clone().detach())                          loss.backward()                          opt.step()                  if i%500==0 or i==0:                 print(f\"Epochs: {epoch:4d} | Iteration: {i:4d}| Loss: {loss.item():4.7f}\")                 print(\"-\"*140)                  In\u00a0[192]: Copied! <pre>train(epochs=2)\n</pre> train(epochs=2) <pre>Epochs:    0 | Iteration:    0| Loss: 2.4361408\n--------------------------------------------------------------------------------------------------------------------------------------------\nEpochs:    0 | Iteration:  500| Loss: 0.4572141\n--------------------------------------------------------------------------------------------------------------------------------------------\nEpochs:    0 | Iteration: 1000| Loss: 0.2291873\n--------------------------------------------------------------------------------------------------------------------------------------------\nEpochs:    0 | Iteration: 1500| Loss: 0.0312376\n--------------------------------------------------------------------------------------------------------------------------------------------\nEpochs:    1 | Iteration:    0| Loss: 0.2579556\n--------------------------------------------------------------------------------------------------------------------------------------------\nEpochs:    1 | Iteration:  500| Loss: 0.1429092\n--------------------------------------------------------------------------------------------------------------------------------------------\nEpochs:    1 | Iteration: 1000| Loss: 0.1931087\n--------------------------------------------------------------------------------------------------------------------------------------------\nEpochs:    1 | Iteration: 1500| Loss: 0.0513597\n--------------------------------------------------------------------------------------------------------------------------------------------\n</pre> In\u00a0[193]: Copied! <pre>with torch.inference_mode():\n    input , label = next(iter(DataLoader(train_data, batch_size=50000)))\n    label_pred = torch.argmax(clf(input), axis=1)\n    print(classification_report(label_pred, label))\n</pre> with torch.inference_mode():     input , label = next(iter(DataLoader(train_data, batch_size=50000)))     label_pred = torch.argmax(clf(input), axis=1)     print(classification_report(label_pred, label)) <pre>              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.96      5056\n           1       0.98      0.95      0.96      5852\n           2       0.92      0.95      0.93      4849\n           3       0.92      0.90      0.91      5229\n           4       0.94      0.95      0.94      4813\n           5       0.90      0.94      0.92      4331\n           6       0.97      0.94      0.96      5084\n           7       0.96      0.94      0.95      5304\n           8       0.86      0.94      0.90      4497\n           9       0.92      0.91      0.92      4985\n\n    accuracy                           0.94     50000\n   macro avg       0.93      0.94      0.94     50000\nweighted avg       0.94      0.94      0.94     50000\n\n</pre> In\u00a0[194]: Copied! <pre>with torch.inference_mode():\n    input , label = next(iter(test_loader))\n    label_pred = torch.argmax(clf(input), axis=1)\n    print(classification_report(label_pred, label))\n</pre> with torch.inference_mode():     input , label = next(iter(test_loader))     label_pred = torch.argmax(clf(input), axis=1)     print(classification_report(label_pred, label)) <pre>              precision    recall  f1-score   support\n\n           0       0.98      0.94      0.96      1052\n           1       0.99      0.93      0.96      1189\n           2       0.91      0.94      0.93       936\n           3       0.92      0.90      0.91      1027\n           4       0.93      0.96      0.94       960\n           5       0.90      0.93      0.92       877\n           6       0.97      0.95      0.96      1028\n           7       0.96      0.94      0.95      1087\n           8       0.85      0.95      0.90       857\n           9       0.91      0.92      0.92       987\n\n    accuracy                           0.94     10000\n   macro avg       0.93      0.94      0.93     10000\nweighted avg       0.94      0.94      0.94     10000\n\n</pre> <p>Generative modeling with AutoEncoders models? how it's possible ?</p> <p>After the model is well trained on enough dataset, with unsupervised approch, we use the Decoder model to generte some new data that have same distribution, leaned during the training process. But, the Decoder accepts as input a $Z$ Embeddings, So how can we find (generate) $Z$? could we? If the answer is yes How? Must be random to genrate some different data at each time.</p> <p>In this section, we'll answer this questions \ud83d\ude01</p> <p>After the model has been thoroughly trained on a sufficient dataset using an unsupervised approach, we utilize the Decoder model to generate new data that follows the same distribution learned during training. However, the Decoder requires a $Z$ Embedding as input. How can we generate this $Z$ embedding? Is it possible to do so? If yes, how can we achieve this in a random manner to produce diverse data each time?</p> <p>In this section, we will address these questions \ud83d\ude01.</p> In\u00a0[47]: Copied! <pre>def get_mean_std_of(digit: int = 0) -&gt; None:\n    digit_set = Subset(mnist, indices= tuple(i for i, cls in enumerate(mnist.targets) if cls==digit))\n    digit_x = torch.stack([d[0].flatten() for d in digit_set])\n\n    emb_x = model.encoder(digit_x)\n\n    return emb_x, emb_x.mean(axis=0), emb_x.std(axis=0)\n</pre> def get_mean_std_of(digit: int = 0) -&gt; None:     digit_set = Subset(mnist, indices= tuple(i for i, cls in enumerate(mnist.targets) if cls==digit))     digit_x = torch.stack([d[0].flatten() for d in digit_set])      emb_x = model.encoder(digit_x)      return emb_x, emb_x.mean(axis=0), emb_x.std(axis=0) In\u00a0[48]: Copied! <pre>def plot_emb_dist_of(digit_emb: torch.Tensor):\n    \n    fig, axes = plt.subplots(20, 5, figsize=(18, 40))\n    for i in range(20):\n        for j in range(5):\n            sns.histplot(ax=axes[i, j], data=digit_emb[:, i + j].detach().numpy(), kde=True)\n    \n    plt.show()\n</pre> def plot_emb_dist_of(digit_emb: torch.Tensor):          fig, axes = plt.subplots(20, 5, figsize=(18, 40))     for i in range(20):         for j in range(5):             sns.histplot(ax=axes[i, j], data=digit_emb[:, i + j].detach().numpy(), kde=True)          plt.show() In\u00a0[49]: Copied! <pre>digit_emb, digit_mean, digit_std = get_mean_std_of(9)\n</pre> digit_emb, digit_mean, digit_std = get_mean_std_of(9) <p>To obtain the <code>digit_emb</code> we fit the digit (in our case, it's 9) to encoder, and get the embeddings $Z$ represents the matrix contains vetors representation of all fitted images of digit 9. after that we compute the <code>mean()</code> and <code>std()</code> respect to axis=0. Formaly,</p> <p>$$ Z = \\begin{bmatrix}         \\cdots &amp; Z^1 &amp; \\cdots \\\\         \\cdots &amp; Z^i &amp; \\cdots \\\\         \\cdots &amp; Z^N &amp; \\cdots \\\\         \\end{bmatrix} $$</p> <p>With $N$ denotes number of images, $Z^i$ is the embeddings vector of the $i$-th image. For each compenents of $Z^i$ we compute the mean and std.</p> In\u00a0[50]: Copied! <pre>digit_emb\n</pre> digit_emb Out[50]: <pre>tensor([[-0.7366, -0.4719, -1.4637,  ...,  0.4467,  0.0897, -0.3719],\n        [-0.7426, -0.2930,  0.5301,  ...,  0.1755,  0.3119, -0.5933],\n        [-0.7640, -0.2663,  0.1523,  ...,  0.8241,  0.5406, -1.1856],\n        ...,\n        [-1.5418,  0.3600,  0.3364,  ...,  0.0881,  0.8023, -0.4454],\n        [-1.3054,  0.1946,  0.4422,  ..., -0.2022,  0.3665, -0.8218],\n        [-1.1912,  0.1435,  0.5495,  ...,  0.0060, -0.0938, -0.6812]],\n       grad_fn=&lt;AddmmBackward0&gt;)</pre> In\u00a0[51]: Copied! <pre>digit_emb.shape\n</pre> digit_emb.shape Out[51]: <pre>torch.Size([5949, 100])</pre> <p>The <code>digit_emb</code> contains 5949 images's embeddings, and each vetor have 100 dimension space.</p> In\u00a0[52]: Copied! <pre>digit_mean\n</pre> digit_mean Out[52]: <pre>tensor([-1.2662, -0.0083, -0.5534,  0.8313, -1.0519,  0.0355, -0.2647,  0.0503,\n        -0.6883, -0.1284, -0.2298, -2.6558, -0.2886,  0.7134, -0.4494, -1.1866,\n        -0.0667,  1.8808, -0.2912,  1.1527,  1.2619, -1.4137,  0.5650, -0.1574,\n        -1.9292,  0.2559,  0.1122,  0.7945, -2.3893,  0.1416, -0.2982,  0.3167,\n        -1.7033, -0.0260, -2.3307,  0.9711, -0.6544, -0.1949,  0.0745, -0.0372,\n         0.2322, -2.0985,  0.5538,  0.4549,  0.5401,  0.3159,  0.2779, -0.6747,\n         0.1522, -2.6267, -0.2635, -1.4348,  0.9232, -0.1682,  0.0187, -0.2834,\n         0.5802, -0.9982,  0.1303,  0.0860,  0.1290, -1.7354,  0.3006, -0.6328,\n         0.3462,  0.1124, -0.8873, -0.3121, -0.0027, -2.2852,  0.7619,  1.2396,\n        -0.5202,  0.1581,  0.4499, -0.3522, -1.9701, -1.7416, -0.0552, -0.3280,\n         0.6288,  0.6407,  0.1948, -0.6247,  1.1016,  0.7926,  1.2080, -0.4066,\n         0.1804,  0.9586, -0.5031,  0.0623, -0.0550,  0.2794, -0.3806,  0.5647,\n        -2.4779,  0.5876, -0.0717, -0.4997], grad_fn=&lt;MeanBackward1&gt;)</pre> In\u00a0[53]: Copied! <pre>digit_std\n</pre> digit_std Out[53]: <pre>tensor([0.5857, 0.4373, 0.6236, 0.5086, 0.8564, 0.4170, 0.3933, 0.5431, 0.6389,\n        0.7187, 0.3988, 0.7239, 0.4142, 0.4460, 0.4337, 0.4392, 0.3893, 0.5184,\n        0.4619, 0.6952, 0.4890, 0.6142, 0.5308, 0.3499, 0.4983, 0.4681, 0.4485,\n        0.3974, 0.9741, 0.5993, 0.6073, 0.4521, 0.6441, 0.5165, 0.7473, 0.5060,\n        0.5504, 0.6165, 0.3680, 0.6795, 0.4656, 0.7644, 0.7590, 0.3959, 0.4244,\n        0.4839, 0.3207, 0.4111, 0.6288, 0.5481, 0.5095, 0.4486, 0.4098, 0.3575,\n        0.5670, 0.5506, 0.5754, 0.5533, 0.4342, 0.8434, 0.4537, 0.6176, 0.5008,\n        0.8992, 0.4078, 0.3981, 0.2968, 0.4570, 0.7115, 0.5865, 0.6330, 0.6505,\n        0.4761, 0.4638, 0.4609, 0.4045, 0.5049, 0.6141, 0.6175, 0.3163, 0.4052,\n        0.5183, 0.7035, 0.4513, 0.4975, 0.4532, 0.6242, 0.5191, 0.4351, 0.6003,\n        0.4255, 0.4084, 0.6498, 0.6538, 0.5847, 0.5047, 0.8519, 0.4414, 0.5818,\n        0.6135], grad_fn=&lt;StdBackward0&gt;)</pre> In\u00a0[54]: Copied! <pre>plot_emb_dist_of(digit_emb)\n</pre> plot_emb_dist_of(digit_emb) <p>The plots above represents the distribution of each components (column vectors) in matrix $Z$, As we can notice, that this components have some kind of normal ditribution, formaly :</p> <p>$$ Z^i_j \\sim \\mathcal{N}(\\mu_j,\\,\\sigma_j^{2})\\, \\mid \\forall j \\in {1, .., M} $$</p> <p>Where, $M$ is the embeddings vector dimension.</p> <p>\ud83d\udea8 Very Important:</p> <p>So we can use this information to generate new embeddings vectors, from normal distribution with corresponding mean and std. Then fitted to the Decoder model, to generate new images, that never seen before.</p> <p>Are you exciting, for generating images ?</p> In\u00a0[55]: Copied! <pre>gen_z = torch.normal(digit_mean, digit_std).view(-1, 100)\n</pre> gen_z = torch.normal(digit_mean, digit_std).view(-1, 100) In\u00a0[56]: Copied! <pre>gen_z\n</pre> gen_z Out[56]: <pre>tensor([[-1.0992, -0.1533, -0.5911,  0.7690, -0.6242, -0.3955, -0.7678,  0.8706,\n         -1.2348,  0.5609,  0.2802, -2.1522, -0.1627,  0.8093, -0.8000, -1.2148,\n          0.4875,  1.9073,  0.5891,  0.9994,  1.3732, -1.3779,  1.8257,  0.3863,\n         -1.6149,  0.1700,  0.8696,  0.9673, -2.2741, -0.7921, -0.8481,  0.4241,\n         -0.5646, -0.4899, -1.8520,  1.3170, -0.6508, -0.6952,  0.1734,  1.1369,\n          0.1059, -2.6506,  0.8315,  0.5664,  0.6197,  0.6036,  0.2836, -0.5433,\n          0.8050, -2.6792, -0.1089, -1.8175,  0.7994,  0.4014, -0.1033,  0.8018,\n         -0.6101, -0.6959,  0.8952, -0.8804,  0.3725, -2.2469, -0.4047, -0.4948,\n          0.2841,  0.0544, -0.6120, -0.3767, -1.8870, -2.6936,  1.4715,  1.3186,\n         -1.2235,  0.5947,  0.4890, -0.6455, -2.4722, -1.0295,  1.2659, -0.7462,\n          1.1401,  1.3221,  0.8662, -0.6891,  0.6284,  0.3139,  1.6403, -0.9198,\n          0.2288,  1.1073, -0.6519, -0.1506, -0.0647, -0.0863,  0.2505, -0.4287,\n         -2.2031,  0.9962, -0.0170, -0.1522]], grad_fn=&lt;ViewBackward0&gt;)</pre> In\u00a0[57]: Copied! <pre>gen_img = model.decoder(gen_z)\n</pre> gen_img = model.decoder(gen_z) In\u00a0[58]: Copied! <pre>imshow_image(gen_img)\n</pre> imshow_image(gen_img) <p>Woooow Great Work \ud83c\udf89\ud83c\udf89\ud83c\udf89\ud83c\udf89\ud83c\udf89\ud83c\udf89, Let do it more one time.</p> In\u00a0[59]: Copied! <pre>gen_other_z = torch.normal(digit_mean, digit_std).view(-1, 100)\n</pre> gen_other_z = torch.normal(digit_mean, digit_std).view(-1, 100) In\u00a0[60]: Copied! <pre>gen_other_img = model.decoder(gen_other_z)\n</pre> gen_other_img = model.decoder(gen_other_z) In\u00a0[61]: Copied! <pre>imshow_image(gen_other_img)\n</pre> imshow_image(gen_other_img)"},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#autoencoders-architecture-in-deeplearning","title":"AutoEncoders Architecture In DeepLearning\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#1-what-is-autoencoders","title":"1. What is AutoEncoders ?\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#2-prepare-the-mnist-dataset","title":"2. Prepare the MNIST Dataset\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#define-some-helping-functions","title":"Define some Helping Functions\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#load-mnsit-dataset","title":"Load MNSIT Dataset\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#split-mnist-dataset","title":"Split MNIST Dataset\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#3-build-the-autoencoders-model","title":"3. Build The AutoEncoders Model\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#define-the-encoder-model","title":"Define The Encoder Model\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#define-the-decoder-model","title":"Define The Decoder Model\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#4-build-the-autoencoder-model","title":"4. Build The AutoEncoder Model:\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#5-train-model","title":"5. Train Model\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#define-loss-and-optimizer","title":"Define Loss and Optimizer\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#lets-training-the-model","title":"Let's Training The Model\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#6-save-models-autoencoder-encoder-decoder","title":"6. Save Models AutoEncoder, Encoder, Decoder\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#lets-load-the-model","title":"Let's Load the Model\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#7-applicattions-of-autoencoders","title":"7. Applicattions of AutoEncoders\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#dimensionality-reduction","title":"Dimensionality Reduction\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#autoencoders-vs-pca","title":"AutoEncoders Vs PCA\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#mnist-classification","title":"MNIST Classification\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#generative-modeling-data-augmentation","title":"Generative modeling &amp; Data Augmentation\u00b6","text":""},{"location":"deep_learning/auto_encoders/AutoEncoders-Architecture-In-DeepLearning/#references","title":"References\u00b6","text":"<ul> <li><p>AutoEncoders Original Paper by G. E. Hinton and R. R. Salakhutdinov Reducing the Dimensionality of Data with Neural Networks</p> </li> <li><p>An Introduction to Autoencoders by Umberto Michelucci</p> </li> </ul>"},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/","title":"Seqeunce Models Specialization with Andrew Ng","text":"<p>This course is notes of Sequence Models created by Andrew Ng.</p> <p>Youtube Link: https://www.youtube.com/watch?v=-YuFMqy2Fq8</p> <p>The vanilla Neural Networks, also known as Multiple Layer Perceptron, assume that inputs are independent of each other, meaning they do not share information between training examples. This assumption works well for data with this property. However, for sequence data like speech or text, MLPs may not model this type of data effectively. MLPs have several weaknesses in this context:</p> <ul> <li>Inputs and outputs can have different lengths in different training examples.</li> <li>MLPs do not share features learned across different positions in the sequence.</li> </ul> <ul> <li>Speech recognition: </li> </ul> <ul> <li>Text -&gt; Sentiment classification: </li> </ul> <ul> <li>DNA sequence analysis </li> </ul> <p>In this section, we will discuss the notations that we will use throughout this course. Let's consider $x$ to be a sequence:</p> <p>$$ x: \\text{Harry Potter and Hermione Granger invented a new spell} $$</p> <p>where each word in the sequence corresponds to a position $t$:</p> <p>$$ \\text{Harry} \\rightarrow x^{&lt;1&gt;} $$</p> <p>$$ \\text{Potter} \\rightarrow x^{&lt;2&gt;} $$</p> <p>$$ \\vdots $$</p> <p>$$ \\text{spell} \\rightarrow x^{&lt;T_x&gt;} $$</p> <p>So, we can map the $i$-th word with $x^{&lt;i&gt;} \\mid i \\in \\{1, \\cdots, T_x \\}$ vector. For label, as well we can map the $j$-th label with encoded label $y^{&lt;j&gt;} \\mid j \\in \\{1, \\cdots, T_y\\}$</p> <p>Let $X$ be a training matrix of size $m \\times n$, where $m$ is the number of training examples, and $n$ represents the number of tokens (words) within each sequence. Therefore,</p> <p>$$ X^{(i)&lt;t&gt;} \\text{ represents the $t$-th token in the $i$-th sequence in the matrix } X. $$</p> <p>The same notation applies for $Y^{(i)&lt;t&gt;}$, which represents the label for each training example.</p> <p>We have seen in previous sections that standard neural network architecture comes with several issues when dealing with sequence data. The Recurrent Neural Network (or RNN) architecture is proposed to solve these problems. The architecture is illustrated in the following figure:</p> <p>Note</p> <p>              One weakness of RNN, we use only the previous information to make predictions, other architecture are proposed to deal with this issue, ex. the BRNN stands for Bidirectional RNN.          </p> <p>Let's define each term in this recurrent architecture. We start with the initial condition: $a^{&lt;0&gt;} = \\vec{0}$. For each time step $t$ from 1 to $T_x = T_y$, we have:</p> <p>$$ a^{&lt;t&gt;} = g \\left( W_{aa}a^{&lt;t-1&gt;} + W_{ax}x^{&lt;t&gt;} + b_a \\right) $$</p> <p>Here, $x^{&lt;t&gt;}$ is the input, and $a^{&lt;t-1&gt;} \\mid_{t=1} = \\vec{0}$. The parameters $W_{aa}$, $W_{ax}$, and $b_a$ are learnable and updated during back-propagation. To compute the predicted $y^{&lt;t&gt;}$, we use:</p> <p>$$ y^{&lt;t&gt;} = h(W_{ya}a^{&lt;t&gt;} + b_y) $$</p> <p>Here, $W_{ya}$ and $b_y$ are learnable parameters, and $a^{&lt;t&gt;}$ is the result of the previous expression. The $g$, $h$ are the activation functions, in practice $g$ is often $tanh$, in other hand the $h$ depends on the task, in classification tasks $\\sigma$ (sigmoid) may be used.</p> <p>Let's now simplify the expressions a lettle bit, we can put :</p> <p>$$ W_a = \\begin{bmatrix}             W_{aa} &amp; W_{ax} \\\\         \\end{bmatrix} \\text{ and }W_{y} = W_{ya} $$</p> <p>so, the previous equations become:</p> <p>$$ a^{&lt;t&gt;} = g \\left( W_a[a^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_a \\right) $$</p> <p>$$ y^{&lt;t&gt;} = h(W_ya^{&lt;t&gt;} + b_y) $$</p> <p>The Recurrent neural networks (RNNs) are flexible in their design and can be adapted to various tasks. This figure illustrates some possible RNN architectures and their corresponding applications:</p> <p>The key thing to remember with RNNs is the difference between the number of inputs $T_x$ and the number of outputs $T_y$. As you'll see, in most RNN architectures, these lengths can be different $T_x \\ne T_y$.</p> <p>The Language Model is a probabilistic model, that assigns probabilities to sequence of words, it's used to estimate the likelihood of a given sequence of tokens (ex. words) occurring in a language. This is typically done by training the model on a large corpus of text (ex. extracted from the net), that try to learn statistical patterns and relationships between different tokens (ex. words).</p> <p>In a Speech Recognition problem where the same sound can be interpreted into multiple words (homophones), language models play a crucial role in determining the most likely interpretation. Consider the example:</p> <ul> <li>$S_1$: The apple and pair salad.</li> <li>$S_2$: The apple and pear salad.</li> </ul> <p>Both pair and pear sound the same, but the second sentence is more likely the correct one. A language model assigns probabilities to sentences, which can help in deciding between these interpretations. For example:</p> <p>$$P(S_1) = 3.2 \\times 10^{-13}$$ $$P(S_2) = 5.7 \\times 10^{-10}$$</p> <p>In this case, the higher probability for $S_2$ indicates that it is more likely the correct interpretation. For pair word in the $S_1$:</p> <p>$$ P(\\text{pair}) = P(\\text{the})P(\\text{apple} \\mid \\text{the})P(\\text{and} \\mid \\text{the, apple})P(\\text{pair} \\mid \\text{the, apple, and})$$</p> <p>This formula is optained using the Law of total probability. The following figure illustrates how you can search for $S_1$ on Google and receive suggestions that indicate the second sentence is make more sense.</p> <p>To build a language model using RNNs, you need to follow several steps:</p> <ol> <li>Prepare a large corpus of text, which can be found online (e.g., Wikipedia).</li> <li>Tokenize this text into tokens. Initially, you can use a naive tokenizer, such as splitting based on whitespace or considering each character as a token. In the comming example, we will build a Character-Level Language Model in PyTorch trained on Shakespeare text.</li> <li>The training set will have the following property:</li> </ol> <p>$$ X^{&lt;t&gt;} = y^{&lt;t-1&gt;}$$</p> In\u00a0[1]: Copied! <pre># import pytorch libray\n\nimport torch, torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n</pre> # import pytorch libray  import torch, torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader, TensorDataset <p>For donwloading the file, check the link shakespeare.txt</p> In\u00a0[2]: Copied! <pre># read the shakespeare's text, and keep only 10000 character\n\ntext = open(\"./shakespeare.txt\", \"r\").read()[:10000]\n</pre> # read the shakespeare's text, and keep only 10000 character  text = open(\"./shakespeare.txt\", \"r\").read()[:10000] In\u00a0[8]: Copied! <pre># print the first 250 characters\n\nprint(text[:250])\n</pre> # print the first 250 characters  print(text[:250]) <pre>First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n</pre> <p>As previously mentioned, we need to construct a vocabulary that includes all unique characters found in our text. The vocabulary contains <code>57</code> characters.</p> In\u00a0[10]: Copied! <pre>vocab = sorted(set(text))\n</pre> vocab = sorted(set(text)) In\u00a0[13]: Copied! <pre>print(vocab)\n</pre> print(vocab) <pre>['\\n', ' ', '!', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n</pre> In\u00a0[15]: Copied! <pre># the length of vocabulary:\n\nprint(len(vocab))\n</pre> # the length of vocabulary:  print(len(vocab)) <pre>57\n</pre> <p>The <code>encoder</code> and <code>decoder</code> dictionaries are required. The <code>encoder</code> indexes the characters with unique IDs, while the <code>decoder</code> takes an ID and returns the corresponding character.</p> In\u00a0[16]: Copied! <pre>encoder = {char:i for i, char in enumerate(vocab)}\ndecoder = {i:char for i, char in enumerate(vocab)}\n</pre> encoder = {char:i for i, char in enumerate(vocab)} decoder = {i:char for i, char in enumerate(vocab)} In\u00a0[19]: Copied! <pre>print(encoder)\n</pre> print(encoder) <pre>{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, ',': 4, '-': 5, '.': 6, ':': 7, ';': 8, '?': 9, 'A': 10, 'B': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15, 'H': 16, 'I': 17, 'J': 18, 'L': 19, 'M': 20, 'N': 21, 'O': 22, 'P': 23, 'R': 24, 'S': 25, 'T': 26, 'U': 27, 'V': 28, 'W': 29, 'Y': 30, 'a': 31, 'b': 32, 'c': 33, 'd': 34, 'e': 35, 'f': 36, 'g': 37, 'h': 38, 'i': 39, 'j': 40, 'k': 41, 'l': 42, 'm': 43, 'n': 44, 'o': 45, 'p': 46, 'q': 47, 'r': 48, 's': 49, 't': 50, 'u': 51, 'v': 52, 'w': 53, 'x': 54, 'y': 55, 'z': 56}\n</pre> In\u00a0[20]: Copied! <pre>print(decoder)\n</pre> print(decoder) <pre>{0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: ',', 5: '-', 6: '.', 7: ':', 8: ';', 9: '?', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'H', 17: 'I', 18: 'J', 19: 'L', 20: 'M', 21: 'N', 22: 'O', 23: 'P', 24: 'R', 25: 'S', 26: 'T', 27: 'U', 28: 'V', 29: 'W', 30: 'Y', 31: 'a', 32: 'b', 33: 'c', 34: 'd', 35: 'e', 36: 'f', 37: 'g', 38: 'h', 39: 'i', 40: 'j', 41: 'k', 42: 'l', 43: 'm', 44: 'n', 45: 'o', 46: 'p', 47: 'q', 48: 'r', 49: 's', 50: 't', 51: 'u', 52: 'v', 53: 'w', 54: 'x', 55: 'y', 56: 'z'}\n</pre> In\u00a0[21]: Copied! <pre>X = torch.tensor([encoder[char] for char in text[:-1]])\n</pre> X = torch.tensor([encoder[char] for char in text[:-1]]) In\u00a0[22]: Copied! <pre>X[:10]\n</pre> X[:10] Out[22]: <pre>tensor([15, 39, 48, 49, 50,  1, 12, 39, 50, 39])</pre> In\u00a0[23]: Copied! <pre>X.shape\n</pre> X.shape Out[23]: <pre>torch.Size([9999])</pre> In\u00a0[25]: Copied! <pre>Y = torch.tensor([encoder[char] for char in text[1:]])\n</pre> Y = torch.tensor([encoder[char] for char in text[1:]]) In\u00a0[27]: Copied! <pre>Y[:10]\n</pre> Y[:10] Out[27]: <pre>tensor([39, 48, 49, 50,  1, 12, 39, 50, 39, 56])</pre> In\u00a0[28]: Copied! <pre># create the dataset using TensorDataset\n\ntrain_data = TensorDataset(X, Y)\n</pre> # create the dataset using TensorDataset  train_data = TensorDataset(X, Y) In\u00a0[30]: Copied! <pre>train_data[:4]\n</pre> train_data[:4] Out[30]: <pre>(tensor([15, 39, 48, 49]), tensor([39, 48, 49, 50]))</pre> In\u00a0[31]: Copied! <pre># define a train loader with batch_size 500\n\ntrain_loader = DataLoader(train_data, batch_size=500)\n</pre> # define a train loader with batch_size 500  train_loader = DataLoader(train_data, batch_size=500) In\u00a0[32]: Copied! <pre>class LM_RNNModel(nn.Module):\n    \n    vocab_size: int\n    emb_size: int = 32\n    hidden_size: int = 128\n    num_layers: int = 1\n    seq_size: int = 1\n    \n    def __init__(self, vocab_size: int) -&gt; None:\n        super(LM_RNNModel, self).__init__()\n        self.vocab_size: int = vocab_size\n        \n        self.emb = nn.Embedding(self.vocab_size, self.emb_size)\n\n        self.rnn = nn.RNN(self.emb_size, self.hidden_size, self.num_layers)\n\n\n        self.fc = nn.Sequential(\n            nn.Linear(self.hidden_size, 64),\n            nn.Linear(64, self.vocab_size)\n        )\n            \n\n    def forward(self, input: torch.tensor, hidden: torch.tensor) -&gt; torch.tensor:\n        out = self.emb(input)\n        out , hidden = self.rnn(out, hidden)\n        return self.fc(out), hidden\n</pre> class LM_RNNModel(nn.Module):          vocab_size: int     emb_size: int = 32     hidden_size: int = 128     num_layers: int = 1     seq_size: int = 1          def __init__(self, vocab_size: int) -&gt; None:         super(LM_RNNModel, self).__init__()         self.vocab_size: int = vocab_size                  self.emb = nn.Embedding(self.vocab_size, self.emb_size)          self.rnn = nn.RNN(self.emb_size, self.hidden_size, self.num_layers)           self.fc = nn.Sequential(             nn.Linear(self.hidden_size, 64),             nn.Linear(64, self.vocab_size)         )                   def forward(self, input: torch.tensor, hidden: torch.tensor) -&gt; torch.tensor:         out = self.emb(input)         out , hidden = self.rnn(out, hidden)         return self.fc(out), hidden In\u00a0[74]: Copied! <pre># create an instance of LM_RNNModel \n\nmodel = LM_RNNModel(vocab_size=len(vocab))\n</pre> # create an instance of LM_RNNModel   model = LM_RNNModel(vocab_size=len(vocab)) In\u00a0[75]: Copied! <pre>model\n</pre> model Out[75]: <pre>LM_RNNModel(\n  (emb): Embedding(57, 32)\n  (rnn): RNN(32, 128)\n  (fc): Sequential(\n    (0): Linear(in_features=128, out_features=64, bias=True)\n    (1): Linear(in_features=64, out_features=57, bias=True)\n  )\n)</pre> <p>The <code>LM_RNNModel</code> contains three main parts. An <code>nn.Embedding</code> layer that map each charater with a vector. Then, a RNN Layer that take two inputs, the character representation and the hidden tensor. Finally, a Full-Connected layer.</p> In\u00a0[76]: Copied! <pre># define the hidden tensor:\n\nhidden = torch.randn(model.seq_size, model.hidden_size)\n</pre> # define the hidden tensor:  hidden = torch.randn(model.seq_size, model.hidden_size) In\u00a0[77]: Copied! <pre>x1, y1 = next(iter(train_loader))\n</pre> x1, y1 = next(iter(train_loader)) In\u00a0[78]: Copied! <pre>model(x1, hidden)\n</pre> model(x1, hidden) Out[78]: <pre>(tensor([[-7.4695e-02, -1.6429e-01,  1.8381e-02,  ...,  2.6543e-01,\n          -1.3445e-04,  3.2170e-01],\n         [-9.5370e-02, -6.8306e-03, -1.1801e-01,  ..., -2.9214e-02,\n          -9.9421e-02, -1.4067e-01],\n         [-2.0766e-02,  6.9504e-02, -1.1826e-01,  ...,  2.5331e-01,\n          -1.0180e-01, -7.5558e-02],\n         ...,\n         [ 7.4759e-03, -9.2480e-02, -1.8126e-01,  ...,  1.4836e-01,\n           3.6416e-02, -1.0179e-01],\n         [-7.2682e-02, -6.5333e-02, -7.8535e-02,  ...,  1.5850e-01,\n          -4.5765e-02, -7.2717e-02],\n         [-7.4645e-02, -6.4154e-02, -3.1239e-02,  ...,  2.6167e-01,\n          -1.1703e-01, -2.6830e-02]], grad_fn=&lt;AddmmBackward0&gt;),\n tensor([[-0.2144,  0.2201, -0.1213,  0.5824, -0.0648, -0.2723,  0.0505, -0.0239,\n           0.1193, -0.3353,  0.0263,  0.1349, -0.3484,  0.1974,  0.6331, -0.1926,\n           0.2692, -0.2282, -0.1555, -0.3032, -0.0698, -0.1179,  0.0994, -0.0680,\n          -0.2117, -0.3542,  0.3325,  0.2934, -0.1904, -0.0686, -0.1570, -0.4542,\n           0.1945, -0.0554,  0.1632,  0.2871, -0.1734,  0.0457, -0.3414,  0.0033,\n           0.1581,  0.2157, -0.3350, -0.3259, -0.2573, -0.0186,  0.1025,  0.1335,\n           0.0591,  0.4304,  0.3505, -0.1662,  0.3837,  0.0182, -0.0783,  0.1007,\n           0.4009, -0.0302,  0.6607, -0.3394,  0.0632,  0.3232, -0.0162, -0.0422,\n          -0.4582,  0.2549, -0.1956, -0.1426, -0.0479,  0.2110,  0.2001,  0.0961,\n           0.4370, -0.1792,  0.0373,  0.1615,  0.0904,  0.0452,  0.1857, -0.4015,\n          -0.0597,  0.0763, -0.4179, -0.1907,  0.1128, -0.1836, -0.0584, -0.0686,\n          -0.3530,  0.1223, -0.1455, -0.4795,  0.1592,  0.5148,  0.2281,  0.0577,\n           0.3594, -0.2124, -0.0072, -0.1246,  0.0613,  0.2290,  0.3373,  0.0193,\n           0.1002,  0.1469,  0.3085, -0.1485, -0.4218,  0.6327,  0.5685,  0.3380,\n           0.1430,  0.0322, -0.2539, -0.3351,  0.4093,  0.0873,  0.2335,  0.1327,\n          -0.4007, -0.0987, -0.2810,  0.0473,  0.0097,  0.2616,  0.1913, -0.0123]],\n        grad_fn=&lt;SqueezeBackward1&gt;))</pre> <p>Let's define a <code>generate</code> function that take the model as parameter and try to generate sequence of charaters</p> In\u00a0[79]: Copied! <pre>def generate(model=model, start_char: str = \"a\", max_token: int = 30) -&gt; None:\n    hidden = torch.randn(model.seq_size, model.hidden_size)\n\n    char = start_char\n\n    print(char, end=\"\")\n    \n    for t in range(max_token):\n\n        input = torch.tensor(encoder[char]).view(-1)\n        \n        out, hidden = model(input, hidden)\n\n        next_index = int(out.argmax())\n\n        char = decoder[next_index]\n\n        print(char, end=\"\")\n</pre> def generate(model=model, start_char: str = \"a\", max_token: int = 30) -&gt; None:     hidden = torch.randn(model.seq_size, model.hidden_size)      char = start_char      print(char, end=\"\")          for t in range(max_token):          input = torch.tensor(encoder[char]).view(-1)                  out, hidden = model(input, hidden)          next_index = int(out.argmax())          char = decoder[next_index]          print(char, end=\"\") <p>Let's generate ten charaters, which start with <code>h</code>:</p> In\u00a0[80]: Copied! <pre>generate(model, start_char=\"h\", max_token=10)\n</pre> generate(model, start_char=\"h\", max_token=10) <pre>hfhteof?LPx</pre> <p>The output doesn't make sense, right ? this model is not tranined yet, wait until the end \ud83d\ude0a.</p> In\u00a0[81]: Copied! <pre># define the cross entropy loss (mutli-classe task)\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # define the cross entropy loss (mutli-classe task)  criterion = nn.CrossEntropyLoss() In\u00a0[82]: Copied! <pre># define AdamW optimizer with lr=0.01\n\nopt = optim.AdamW(model.parameters(), lr = 0.001)\n</pre> # define AdamW optimizer with lr=0.01  opt = optim.AdamW(model.parameters(), lr = 0.001) In\u00a0[83]: Copied! <pre>def train(epochs: int = 100): \n    hidden = torch.randn(model.seq_size, model.hidden_size)\n    \n    for epoch in range(epochs):\n        \n        for i, (inputs, labels) in enumerate(train_loader):\n        \n            opt.zero_grad()\n\n            hidden = hidden.detach()\n            \n            labels_pred, hidden = model(inputs, hidden)\n        \n            loss = criterion(labels_pred, labels)\n            \n            loss.backward()\n            \n            opt.step()\n    \n        if epoch%10==0 or epoch==0:\n            print(f\"Epochs: {epoch:4d} | Iteration: {i:4d}| Loss: {loss.item():4.7f}\")\n            print(\"-\"*80)\n</pre> def train(epochs: int = 100):      hidden = torch.randn(model.seq_size, model.hidden_size)          for epoch in range(epochs):                  for i, (inputs, labels) in enumerate(train_loader):                      opt.zero_grad()              hidden = hidden.detach()                          labels_pred, hidden = model(inputs, hidden)                      loss = criterion(labels_pred, labels)                          loss.backward()                          opt.step()              if epoch%10==0 or epoch==0:             print(f\"Epochs: {epoch:4d} | Iteration: {i:4d}| Loss: {loss.item():4.7f}\")             print(\"-\"*80)                  In\u00a0[84]: Copied! <pre>train()\n</pre> train() <pre>Epochs:    0 | Iteration:   19| Loss: 3.2646050\n--------------------------------------------------------------------------------\nEpochs:   10 | Iteration:   19| Loss: 2.2732933\n--------------------------------------------------------------------------------\nEpochs:   20 | Iteration:   19| Loss: 1.9699154\n--------------------------------------------------------------------------------\nEpochs:   30 | Iteration:   19| Loss: 1.7258064\n--------------------------------------------------------------------------------\nEpochs:   40 | Iteration:   19| Loss: 1.5044402\n--------------------------------------------------------------------------------\nEpochs:   50 | Iteration:   19| Loss: 1.2867706\n--------------------------------------------------------------------------------\nEpochs:   60 | Iteration:   19| Loss: 1.1347806\n--------------------------------------------------------------------------------\nEpochs:   70 | Iteration:   19| Loss: 0.9803897\n--------------------------------------------------------------------------------\nEpochs:   80 | Iteration:   19| Loss: 0.8625445\n--------------------------------------------------------------------------------\nEpochs:   90 | Iteration:   19| Loss: 0.8428609\n--------------------------------------------------------------------------------\n</pre> <p>After the <code>LM_RNNModel</code> model is trained, let's generate some ShakeSpeare style \ud83d\ude03 \ud83d\ude05.</p> In\u00a0[85]: Copied! <pre>generate(model, start_char=\"h\", max_token=1000)\n</pre> generate(model, start_char=\"h\", max_token=1000) <pre>hen toud course tort as make it, en thuth are from they were are all in counde though sends you think wele, thing lation, will feriming tur ourt, the are and if they wall one acun to the sences his first the ristrongest thich are this deart will flatter\nUnto the anowe\nAs, ling tour soffor us not undo food\n'ffmous,\nOr this forthith,\nWiet what to fines from thes way thing link noth o't make it, en thuth are from they were are all in counde though sends you think wele, thing lation, will feriming tur ourt, the are and if they wall one acun to the sences his first the ristrongest thich are this deart will flatter\nUnto the anowe\nAs, ling tour soffor us not undo food\n'ffmous,\nOr this forthith,\nWiet what to fines from thes way thing link noth o't make it, en thuth are from they were are all in counde though sends you think wele, thing lation, will feriming tur ourt, the are and if they wall one acun to the sences his first the ristrongest thich are this deart will flatter\nUnto the anowe\nAs, li</pre> <p>The Vanishing Gradients problem occurs when gradients become extremely small as they are backpropagated through the layers of a deep neural network during training. This the biggest problem of basic RNNs.</p> <p>Let's take an example. Suppose we are working with language modeling problem and there are two sequences that model tries to learn:</p> <ul> <li>The cat, which already ate ..., was full.</li> <li>The cats, which already ate ..., were full.</li> </ul> <p>What we need to learn here that was came with cat and that were came with cats. The naive RNN is not very good at capturing very long-term dependencies like this. So, architectures like LSTM (Long Short Term Memory) and GRU(Gated Recurrent Unit) are proposed to solve this issue.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#seqeunce-models-specialization-with-andrew-ng","title":"Seqeunce Models Specialization with Andrew Ng\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#1-why-sequence-models","title":"1. Why Sequence Models ?\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#can-we-use-mlp-for-sequence-data","title":"Can we use MLP for sequence Data?\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#example-of-sequence-data","title":"Example of Sequence Data\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#notations","title":"Notations\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#2-recurrent-neural-networks-models","title":"2. Recurrent Neural Networks Models\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#forward-propagation","title":"Forward Propagation\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#rnn-architectures","title":"RNN Architectures\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#language-models-and-sequence-generation","title":"Language Models and Sequence Generation\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#whats-language-model-lm","title":"What's Language Model (LM)\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#how-build-a-language-model-using-rnns","title":"How Build a Language Model using RNNs\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#character-level-generation-with-rnns-using-pytorch","title":"Character Level Generation with RNNs using PyTorch\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#prepare-a-large-corpus-text-of-shakespeare","title":"Prepare a large corpus Text of ShakeSpeare\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#prepare-the-training-set","title":"Prepare the Training set\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#define-the-language-model-rnn-architecture","title":"Define The Language Model RNN Architecture\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#time-to-train","title":"Time to Train \ud83d\ude0a\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#vanishing-gadients-with-rnns","title":"Vanishing Gadients With RNNs\u00b6","text":""},{"location":"deep_learning/seq_models/Seqeunce-Models-Specialization-with-Andrew-Ng/#references","title":"References\u00b6","text":"<ul> <li>The Unreasonable Effectiveness of Recurrent Neural Networks http://karpathy.github.io/2015/05/21/rnn-effectiveness/</li> <li>Other Notes for this course : coursera deep learning specialization sequence models</li> </ul>"},{"location":"llms/","title":"Large Language Models","text":""},{"location":"llms/attention-is-all-you-need/","title":"Attention Is All You Need","text":"<p>In 2017, Google researchers published a paper titled Attention Is All You Need, introducing a novel architecture called the Transformer in the context of machine translation tasks. This architecture has significantly influenced AI research and inspired many other applications.</p> <p>The Transformer is at the heart of modern Large Language Models (LLMs), the most famous of which is ChatGPT from OpenAI. ChatGPT is based on the GPT architecture, which stands for Generative Pre-trained Transformer and serves as an assistant chatbot. Another notable application of the Transformer architecture is BERT, described in the BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding paper. The BERT model is used in many tasks like sentiment analysis, question answering (Q&amp;A), and more.</p> <p>This tutorial will delve deep into the Attention Is All You Need paper, exploring the Transformer architecture in depth, from theory to code.</p> Tranformer Architecture <p>Recurrent neural networks (RNNs) were once the state-of-the-art for sequence-to-sequence modeling. They are now used for handling sequence data in language modelling, speech recognition, machine translation, and more.</p> Recurrent Neural Networks Architecture <p>RNNs are quite effective for handling small sequences. However, they lose efficiency when dealing with large sequences. To address this, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), which are variants of RNNs, were introduced to tackle the vanishing gradients issue that arises with longer sequences. Despite these advancements, all RNN architectures still suffer from several issues:</p> <ul> <li><p>RNNs are slow: Due to their sequential nature, RNNs process inputs one at a time to generate the output. For example, to process the third token in a sequence, we must first process the first and second tokens. This sequential dependency makes both training and inference slow.</p> </li> <li><p>Gradient issues: Vanishing Gradient and Exploding Gradient are significant problems with RNNs. Although LSTMs and GRUs were designed to mitigate these issues, they remain unavoidable when dealing with long contexts and sequences.</p> </li> <li><p>Context Representations: The intermediate activation vectors passed from the previous hidden state $ h_t-1$ to the current hidden state $ h_t$ are meant to carry information from previous tokens. However, whether they accurately represent the context is uncertain, especially with long dependencies where information tends to be lost.</p> </li> </ul> <p>In the next sections, we will explore how Transformers address these issues using a clever trick called the Attention Mechanism.</p> <p>First of all, before we dive into understanding what tokenization is, I want to mention that Andrej Karpathy created a great video on this topic.</p> <p>You can find it here https://www.youtube.com/watch?v=zduSFxRajkE.</p>  Let's build the GPT Tokenizer  <p>In simple terms, Tokenization is breaking sequences into smaller tokens. These tokens can vary in size, ranging from single letters to sub-words, words, or even multiple words. Each token is assigned an integer representing its unique identifier, or <code>id</code>.</p> <p>The Tokenization stage is very critical before plugging text sequences naively into the language models; without it, they won't work.</p> <p>You can actually test the tokenization online by visiting the following website: tiktokenizer. After entering some text, you can see that the sequence is broken into tokens on the right, with different colors.</p> Tiktoken Vercel App <p>Question</p> <p>     A very good question that can be asked at this moment is, why don't we just consider words as tokens, separated by white space? </p> <p>It's very convenient and logical to consider why complicate things by breaking words into sub-words, etc.</p> <p>In simple terms, if we consider tokens as just words separated by white space, we'll encounter a couple of issues:</p> <ul> <li><p>Firstly, the text sequences we work with are not always written in a high-quality format. For example, if the sequence contains something like <code>Attention isallyou need,</code> we would consider <code>isallyou</code> as one word, but it's three different words. Therefore, we need a clever way to tokenize the sequence.</p> </li> <li><p>Secondly, tokenization determines the vocabulary, which consists of unique tokens. If we consider tokens just words, we end up with a huge vocabulary size, potentially in the order of hundreds of thousands or even millions. This can be challenging when dealing with language modeling. However, if we consider tokens as just characters, we end up with a vocabulary size of around a hundred. This is not sufficient for effective language modeling. Therefore, the solution is to find a trade-off between the two.</p> </li> </ul> <p>In the tutorial Let's build the GPT Tokenizer, Andrej Karpathy raises some issues with LLMs due to bad Tokenization, here are some of them:</p> <ul> <li>Why can't LLM spell words?</li> <li>Why can't LLM perform super simple string processing tasks like reversing a string?</li> <li>Why is LLM worse at non-English languages (e.g. Japanese)?</li> <li>Why is LLM bad at simple arithmetic?</li> <li>Why did GPT-2 have more than necessary trouble coding in Python?</li> </ul> <p>In fact, there are many tokenizers available, and each LLM comes with its own tokenizer. For OpenAI, it uses tiktoken for their GPT LLMs, which is based on the Byte Pair Encoding algorithm. Google uses SentencePiece, an unsupervised text tokenizer.</p> <p>In this section, we're going to use a toy examples with tiktoken.</p> <p>First, let's install the <code>tiktoken</code> library.</p> In\u00a0[4]: Copied! <pre>!pip install tiktoken\n</pre> !pip install tiktoken <pre>Defaulting to user installation because normal site-packages is not writeable\nCollecting tiktoken\n  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/61/b4/b80d1fe33015e782074e96bbbf4108ccd283b8deea86fb43c15d18b7c351/tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex&gt;=2022.1.18 in /home/moussa/.local/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\nRequirement already satisfied: requests&gt;=2.26.0 in /usr/lib/python3/dist-packages (from tiktoken) (2.31.0)\nDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 2.2 MB/s eta 0:00:00m eta 0:00:01[36m0:00:010m\nInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.7.0\n</pre> <p>Next, import the library and load the tokenizer for <code>gpt2</code>.</p> In\u00a0[5]: Copied! <pre>import tiktoken\n\nencoder = tiktoken.get_encoding(\"gpt2\")\n</pre> import tiktoken  encoder = tiktoken.get_encoding(\"gpt2\") In\u00a0[6]: Copied! <pre>encoder\n</pre> encoder Out[6]: <pre>&lt;Encoding 'gpt2'&gt;</pre> <p>Now, let's use the <code>gpt2</code> tokenizer to encode the text <code>hello everyone</code> and then decode it.</p> In\u00a0[22]: Copied! <pre>code = encoder.encode(\"hello everyone\")\ncode\n</pre> code = encoder.encode(\"hello everyone\") code Out[22]: <pre>[31373, 2506]</pre> In\u00a0[23]: Copied! <pre>[encoder.decode([c]) for c in code]\n</pre> [encoder.decode([c]) for c in code] Out[23]: <pre>['hello', ' everyone']</pre> <p>Nowadays, AI technologies are used no matter the data type and task you have. The reason behind it from my perspective, is Embeddings.</p> <p>Embeddings are numerical representations of real-world objects, and data. They are used to capture the semantic and Syntatic relationships, and then later these embeddings are used by ML/AI systems to learn more complex patterns, and relationships.</p> Vector Embeddings <p>Before, starting to dive into the details, Here are two blogs for further information.</p> <ul> <li>Amazon AWS : What Are Embeddings In Machine Learning?.</li> <li>OpenAI: Introducing text and code embeddings</li> </ul> <p>Well, the short answer is we cannot plug data, i.e. text, images, audio, etc, into machine learning algorithms as they are. Instead, a preprocessing stage is needed, in which data is converted into some numerical representations, Embeddings.</p> <p>That means machine learning algorithms can be used for different purposes and handling data no matter their type.</p> <p>Embeddings are used for many purposes, let's explore some of them.</p> <ul> <li><p>Dimensionality Reduction: In that case, embeddings are used to encode and compress some high-dimension data into low-dimension space. Which helps a lot to reduce complexity, and makes algorithms fast in training and inference.</p> </li> <li><p>Large Language Models: Retrieve Augmented Generation (RAG) helps LLMs generate more facts by providing them with factual inputs, similar to the user's query, for this similarity we use embeddings.</p> </li> </ul> <p>For further information about RAG, I suggest checking a great tutorial Local Retrieval Augmented Generation (RAG) from Scratch made by Daniel Bourke.</p> <p>Embeddings are created, by training Neural Networks to encode the information into dense representation in a multi-dimention space.</p> <p>Let's explore some neural network architectures used for learning embeddings for text data.</p> <ul> <li><p>Word2Vec: This architecture is proposed by Google in two versions, Continuous Bag of Words (CBOW) and Skip-Gram. With the CBOW method, embeddings are learned by performing classification tasks. Given a context size, called window size <code>w</code>, we predict the word in the middle.</p> </li> <li><p>BERT: is a Transformer-based architecture. It is trained on massive data to understand language by performing two tasks: Language Masking and Next Sentence Prediction.</p> </li> </ul> <p>A simple difference between Word2Vec and BERT, is that Word2Vec cannot distinguish contextual differences of the same word used to imply different meanings. On the other hand, BERT can change the embeddings of a token (word) depending on the context.</p> <p>Let's explore the word embeddings in code. First, we import <code>gensim</code> library will provide pre-trained word embeddings, like <code>glove-twitter-25</code> that encode each word into a dense vector of 25 dimensions.</p> In\u00a0[1]: Copied! <pre>import gensim.downloader\nimport numpy as np\n</pre> import gensim.downloader import numpy as np In\u00a0[2]: Copied! <pre>glove = gensim.downloader.load('glove-twitter-25')\n</pre> glove = gensim.downloader.load('glove-twitter-25') <p>After downloading and load the <code>glove-twitter-25</code> word embeddings model, let's find the most similar to word king.</p> In\u00a0[3]: Copied! <pre>glove.most_similar(\"king\")\n</pre> glove.most_similar(\"king\") Out[3]: <pre>[('prince', 0.9337409734725952),\n ('queen', 0.9202421307563782),\n ('aka', 0.9176921844482422),\n ('lady', 0.9163240790367126),\n ('jack', 0.9147354364395142),\n (\"'s\", 0.9066898226737976),\n ('stone', 0.8982374668121338),\n ('mr.', 0.8919409513473511),\n ('the', 0.889343798160553),\n ('star', 0.8892088532447815)]</pre> <p>Beautiful, we can see that prince and queen are on top of the list. That shows how these vector representations capture the meaning of the word.</p> <p>Note</p> <p>        We can see on the list that, 's is similar to the word king with 90.66%. Is not make sense, right?      </p> <p>         The raison is that the quality of the embeddings depends on the quality of data, the NNs architecture used and more, as you can notice that the data used for the training comes form the twitter. That explains why we get this results.     </p> <p>Let's define the cosine similarity metric that measure the angle between two vectors.</p> In\u00a0[4]: Copied! <pre>def cosine_similarity(vec1, vec2):\n    return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))\n</pre> def cosine_similarity(vec1, vec2):     return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2)) <p>Let's have fun and play with some embeddings vectors, to replicate the famous equation <code>king = queen + man - woman</code> and see what we can get.</p> In\u00a0[15]: Copied! <pre>man = glove.get_vector(\"man\")\nwoman = glove.get_vector(\"woman\")\n</pre> man = glove.get_vector(\"man\") woman = glove.get_vector(\"woman\") In\u00a0[16]: Copied! <pre>king = glove.get_vector(\"king\")\nqueen = glove.get_vector(\"queen\")\n</pre> king = glove.get_vector(\"king\") queen = glove.get_vector(\"queen\") <p>First, let's take look to the embeddings of the word <code>king</code></p> In\u00a0[12]: Copied! <pre>king\n</pre> king Out[12]: <pre>array([-0.74501 , -0.11992 ,  0.37329 ,  0.36847 , -0.4472  , -0.2288  ,\n        0.70118 ,  0.82872 ,  0.39486 , -0.58347 ,  0.41488 ,  0.37074 ,\n       -3.6906  , -0.20101 ,  0.11472 , -0.34661 ,  0.36208 ,  0.095679,\n       -0.01765 ,  0.68498 , -0.049013,  0.54049 , -0.21005 , -0.65397 ,\n        0.64556 ], dtype=float32)</pre> <p>As you can see the token (word) king, is represented as a dense vector of type <code>float32</code>. With this representation, we can now compute the similarity between different embeddings, the following code snippet calculates the cosine similarity between <code>king</code> and <code>man</code> embeddings.</p> In\u00a0[13]: Copied! <pre>cosine_similarity(man, king)\n</pre> cosine_similarity(man, king) Out[13]: <pre>0.7666067</pre> In\u00a0[14]: Copied! <pre>cosine_similarity(queen + man - woman , king)\n</pre> cosine_similarity(queen + man - woman , king) Out[14]: <pre>0.7310066</pre> <p>As seen, in the section about tokenization, it is a process of breaking sequences into small pieces called tokens. As we covered many of the issues with LLMs come from bad tokenization.</p> <p>Now, after breaking the sequences into a list of tokens, we cannot directly plug them into a model as they are, because models understand only numbers, and vectors, i.e. Embeddings.</p> <p>So, the process is too simple, we map each token with embedding representations, that capture the semantic meaning of that specific token. These Embeddings are randomly initialized in the first place but will be learned during the training.</p> From Tokenization to Embeddings <p>At last, we have arrived at this place, to talk about Transformers, I think we have covered all we need to start this topic. So, in the next sections, we're trying to explain all the building blocks of the transformer architecture.</p> <p>Weighted Aggregation is a key concept to fully understand the Attention Mechanism in this section. we're going to cover an inefficient and efficient way to perform aggregation.</p> <p>Aggregation can be defined as the process of collecting data and information from different sources and presenting them in summarized form.</p> <p>This concept can be found in many machine learning algorithms. For instance, consider a single neuron in a multi-layer perceptron (MLP). The output of the neuron is computed as a weighted sum of the input vectors, which is a form of aggregation:</p> <p>$$ z = \\sum_{i} w_i x_i $$</p> <p>Convolutional Neural Networks (CNNs) aggregate information by applying a kernel to an image, combining pixel values from the neighborhood to the center pixel. Similarly, Graph Neural Networks (GNNs) aggregate information through message passing, combining data from neighboring nodes.</p> Convolution operation <p>Before tackling the Weighted Aggregation, we're going step by step to introduce the concept while considering simple examples at the beginning.</p> <p>For instance, consider a list <code>a</code> with three values, we want at each item in the array <code>a</code>. We sum up all items from the beginning to the current item, then return the result as a new array. The computations are illustrated as follows:</p> <p>$$ \\begin{align*} c_0 &amp;= a_0 = 1 \\\\ c_1 &amp;= a_0 + a_1 = 1 + 2 = 3 \\\\ c_2 &amp;= a_0 + a_1 + a_2 = 1 + 2 + 3 = 6 \\end{align*} $$</p> <p>Beautiful, let's see this in code.</p> In\u00a0[1]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[9]: Copied! <pre>a = np.array([1, 2, 3])\n</pre> a = np.array([1, 2, 3]) In\u00a0[10]: Copied! <pre>c = np.array([np.sum(a[:i+1]) for i in range(len(a))])\nc\n</pre> c = np.array([np.sum(a[:i+1]) for i in range(len(a))]) c Out[10]: <pre>array([1, 3, 6])</pre> <p>In the previous example, we used sum aggregation. But what if we want to use a different form of aggregation, such as average? We can achieve this by dividing the sum of the elements by the number of elements.</p> <p>$$ \\begin{align*} c_0 &amp;= \\frac{a_0}{1} = \\frac{1}{1} = 1 \\\\ c_1 &amp;= \\frac{a_0 + a_1}{2} = \\frac{1 + 2}{2} = \\frac{3}{2} = 1.5 \\\\ c_2 &amp;= \\frac{a_0 + a_1 + a_2}{3} = \\frac{1 + 2 + 3}{3} = \\frac{6}{3} = 2 \\end{align*} $$</p> In\u00a0[11]: Copied! <pre>np.array([sum(a[:i+1])/(i+1) for i in range(len(a))])\n</pre> np.array([sum(a[:i+1])/(i+1) for i in range(len(a))]) Out[11]: <pre>array([1. , 1.5, 2. ])</pre> <p>In fact, in the Average Aggregation, we multiply each element with a fix weight $w = \\frac{1}{\\text{num elements}}$, here where the Weighted Aggregation comes, at each iteration $i$ it performs aggregation of element $j$ with $w_{ij}$, and we have</p> <p>$$ \\forall i \\hspace{0.2cm} \\sum_{j}^{}w_{ij} = 1$$</p> <p>$$ \\forall i \\hspace{0.2cm} c_i = \\sum_{j}^{} w_{ij} a_j$$</p> <p>Now, let's see all this in code.</p> In\u00a0[12]: Copied! <pre>a\n</pre> a Out[12]: <pre>array([1, 2, 3])</pre> In\u00a0[13]: Copied! <pre>w = [[1], [0.5, 0.5], [0.1, 0.4, 0.5]]\nw\n</pre> w = [[1], [0.5, 0.5], [0.1, 0.4, 0.5]] w Out[13]: <pre>[[1], [0.5, 0.5], [0.1, 0.4, 0.5]]</pre> In\u00a0[14]: Copied! <pre>c = [ sum(w[i][j] * a[j] for j in range(len(a[:i+1]))) for i in range(len(a))]\nc\n</pre> c = [ sum(w[i][j] * a[j] for j in range(len(a[:i+1]))) for i in range(len(a))] c Out[14]: <pre>[1, 1.5, 2.4]</pre> <p>Question</p> <p>     You may now ask, Why and When we will need this? </p> <p>Great question, the answer is that we're building the concepts that we'll need to understand the Attention Mechanism so make sure that you understand these key ideas.</p> <p>Keep In Mind</p> <p>The most important thing to keep in mind.  Weighted Aggregation  is a way to aggregate previous information, but in a weighted way which means we define how much each element can contribute.      </p> <p>In the previous section we performed, weighted aggregation using for loops which isn't efficient. There is a better way to turn it into matrix multiplication?</p> <p>Hey, what just turning it into matrix multiplication makes this operation efficient?</p> <p>The short answer is yes! I'll explain. Computers excel at performing linear algebra. GPU hardware is specifically optimized to handle matrix multiplication and linear algebra operations with high efficiency.</p> <p>For more information, check out:</p> <ul> <li>Why linear algebra and GPUs are the bedrock of machine learning https://b-yarbrough.medium.com/why-linear-algebra-and-gpus-are-the-bedrock-of-machine-learning-4a55baac2897</li> </ul> <p>Now, let's back to our main topic here.</p> <p>The key idea is to construct the <code>w</code> as the upper triangular portion as shown in the following code snippet.</p> In\u00a0[15]: Copied! <pre>w = np.array([[1  ,   0,   0],\n              [0.5, 0.5,   0],\n              [0.1, 0.4, 0.5]])\nw\n</pre> w = np.array([[1  ,   0,   0],               [0.5, 0.5,   0],               [0.1, 0.4, 0.5]]) w Out[15]: <pre>array([[1. , 0. , 0. ],\n       [0.5, 0.5, 0. ],\n       [0.1, 0.4, 0.5]])</pre> <p>Then multiply this matrix with the <code>a</code> array, which we have before</p> In\u00a0[16]: Copied! <pre>a\n</pre> a Out[16]: <pre>array([1, 2, 3])</pre> In\u00a0[17]: Copied! <pre>w @ a\n</pre> w @ a Out[17]: <pre>array([1. , 1.5, 2.4])</pre> <p>WooW, As we can see. We've just done the same thing as we did before. It is easy to get, right?</p> <p>The last thing I want to explain in this section.</p> <p>What if <code>a</code> is not a vector but a matrix, the aggregation will be performed with respect to rows. This can be done by changing only the value of <code>a</code>, let's have a simple example.</p> In\u00a0[21]: Copied! <pre>a = np.array([[1, 4],\n              [2, 5],\n              [3, 6]])\na\n</pre> a = np.array([[1, 4],               [2, 5],               [3, 6]]) a Out[21]: <pre>array([[1, 4],\n       [2, 5],\n       [3, 6]])</pre> In\u00a0[22]: Copied! <pre># define out\nw = np.array([[1  ,   0,   0],\n              [1/2, 1/2,   0],\n              [1/3, 1/3, 1/3]])\nw\n</pre> # define out w = np.array([[1  ,   0,   0],               [1/2, 1/2,   0],               [1/3, 1/3, 1/3]]) w Out[22]: <pre>array([[1.        , 0.        , 0.        ],\n       [0.5       , 0.5       , 0.        ],\n       [0.33333333, 0.33333333, 0.33333333]])</pre> In\u00a0[23]: Copied! <pre>w @ a\n</pre> w @ a Out[23]: <pre>array([[1. , 4. ],\n       [1.5, 4.5],\n       [2. , 5. ]])</pre> <p>Until now, we've covered many basic concepts but they are essential! such as Tokenization, Embeddings, Weighted Aggregation.</p> <p>Now, we're diving into the Transformer architecture. Initially, we'll explore the Embedding layer and delve into Positional Encoding. Following this, we'll delve into the Attention Mechanism.</p> <p>Embedding Layer is a looking table of dimensions vocabulary size and Embedding Dimension, the embeddings dimension denoted in the original paper as $d_{model} = 512$.</p> Embedding Layer &amp; Positional Encoding <p>This Embedding layer accepts a tenor of indexes (tokens) as input and returns the corresponding embedding vectors. <code>PyTorch</code> has a built-in embedding layer from the <code>nn</code> module.</p> <p>First, we import the <code>torch</code> as <code>torch.nn</code> module, and <code>tiktoken</code> library to use <code>gpt2</code> tokenizer.</p> In\u00a0[71]: Copied! <pre>import torch, torch.nn as nn\nimport tiktoken\n</pre> import torch, torch.nn as nn import tiktoken <p>Let's get the <code>gpt2</code> tokenizer</p> In\u00a0[72]: Copied! <pre>tokenizer = tiktoken.get_encoding(\"gpt2\")\n</pre> tokenizer = tiktoken.get_encoding(\"gpt2\") <p>the <code>gpt2</code> tokenizer has <code>50257</code> in the vocabulary size.</p> In\u00a0[73]: Copied! <pre>tokenizer.n_vocab\n</pre> tokenizer.n_vocab Out[73]: <pre>50257</pre> <p>Now, let's create an instance of the embeddings layer from the <code>nn</code> module, specifying two arguments.</p> <ul> <li><p><code>num_embeddings</code> represents the vocabulary size, and we can set its value equal to <code>tokenizer.n_vocab</code>.</p> </li> <li><p><code>embeddings_dim</code>: represents how many dimensions you want to use to represent tokens.</p> </li> </ul> In\u00a0[74]: Copied! <pre>emb_layer = nn.Embedding(\n    num_embeddings= tokenizer.n_vocab, # voab_size of gpt2 tokenizer\n    embedding_dim= 512 # d_model: embeddings dimension\n)\n</pre> emb_layer = nn.Embedding(     num_embeddings= tokenizer.n_vocab, # voab_size of gpt2 tokenizer     embedding_dim= 512 # d_model: embeddings dimension ) <p>Let's consider that we have the following sentence <code>Hi my name is Moussa</code>, in which we encode using the <code>gpt2</code> tokenizer. then we get the following <code>sequence</code>.</p> In\u00a0[75]: Copied! <pre>sentence = tokenizer.encode(\"Hi my name is Moussa\")\nsentence\n</pre> sentence = tokenizer.encode(\"Hi my name is Moussa\") sentence Out[75]: <pre>[17250, 616, 1438, 318, 42436, 11400]</pre> In\u00a0[76]: Copied! <pre>sequence = torch.LongTensor([sentence]) # LongTensor to ensure that indexes are integers.\nsequence\n</pre> sequence = torch.LongTensor([sentence]) # LongTensor to ensure that indexes are integers. sequence Out[76]: <pre>tensor([[17250,   616,  1438,   318, 42436, 11400]])</pre> In\u00a0[77]: Copied! <pre>sequence.shape\n</pre> sequence.shape Out[77]: <pre>torch.Size([1, 6])</pre> <p>The <code>sequence</code> tensor is of shape <code>(1, 6)</code>, the first dimension represents the batch dimension, here <code>batch = 1</code> because we have a single sentence. The second dimension is the length of your sequence, also called the Time dimension.</p> In\u00a0[78]: Copied! <pre>embeddings = emb_layer(sequence)\nembeddings\n</pre> embeddings = emb_layer(sequence) embeddings Out[78]: <pre>tensor([[[-0.8604,  0.2464,  2.5279,  ..., -1.5272,  1.4942,  0.6481],\n         [ 1.7703, -1.0316,  0.8046,  ...,  0.3494,  0.2895, -1.6490],\n         [-0.7849, -1.7873, -0.4447,  ...,  0.9301,  0.4982, -1.4281],\n         [-0.3122,  0.4844,  0.0659,  ...,  0.0458, -0.3955, -0.0980],\n         [-0.8068, -0.3864,  0.0838,  ..., -1.3389,  0.2242,  0.1750],\n         [-1.1558, -0.2892, -1.2714,  ..., -0.4282, -2.8925, -1.1820]]],\n       grad_fn=&lt;EmbeddingBackward0&gt;)</pre> <p>As you can see, the embeddings layer has grad function<code>grad_fn=&lt;EmbeddingBackward0&gt;</code>, which indicates that it is trainable. These embeddings will be changed and updated during the back-propagation.</p> In\u00a0[79]: Copied! <pre>embeddings.shape\n</pre> embeddings.shape Out[79]: <pre>torch.Size([1, 6, 512])</pre> <p>Let's debug the shape of embeddings with the form of <code>(B, T, d_model)</code>.</p> <ul> <li><p><code>B</code>: represents the batch dimension. how many sentences or sequences do you want to feed the transformer? The batch dimension has a nice property. All sequences are independent of each other, which allows us to process them in parallel and benefit from the computer's power.</p> </li> <li><p><code>T</code>: is the Time dimension, which represents the length of the sequence. How many tokens are in the sequence? The time dimension is considered a batch dimension in the context of using the transformer's architecture (key characteristic), as all tokens are fed at the same time.</p> </li> <li><p><code>d_model</code>: called embeddings dimension, or number of channels. It represents how many dimensions you use to describe the tokens.</p> </li> </ul> <p>In the upcoming section on the Attention Mechanism, we will see that tokens are fed into the transformer architecture in parallel, without performing any recurrent or convolutional operations. This parallel processing is what makes transformer architectures more efficient and faster to train compared to sequential models like RNNs (e.g., GRU and LSTMs).</p> <p>However, this parallel processing introduces a challenge: it doesn't capture the order of tokens (sequence data). Positional Encoding addresses this issue by encoding and mapping each position with a vector of the same dimension as the token embedding dimension (<code>d_model</code>). Before feeding the token into the transformer, we add two vectors: the token embedding and the positional encoding of the corresponding position.</p> <p>Several requirements need to be considered for positional encoding:</p> <ol> <li>The encodings should not depend on the input tokens.</li> <li>The values of the encodings should be small to avoid overshadowing the semantic representation obtained from the token embedding.</li> <li>The encodings should be distinct.</li> </ol> <p>To encode positional information, we need a function that provides values within a close range. The sigmoid function maps any real number to values between 0 and 1:</p> <p>$$     \\forall x \\in \\mathbb{R} \\hspace{0.5cm} \\sigma(x) \\in [0, 1] $$</p> <p>However, the sigmoid function has a limitation: for large values, it converges to 1, which is not suitable for handling long sequences.</p> <p>In the original paper, they proposed using sinusoidal functions because they oscillate between -1 and 1, meeting the requirement of not exceeding the range, even as the input grows infinitely. Therefore, the positional encoding is represented as a matrix $PE \\in \\mathbb{R}^{n \\times d\\_model}$, defined as:</p> <p>$$ \\forall i \\in \\left[0, \\frac{d\\_model}{2}\\right] \\left\\{ \\begin{align*} PE_{pos, 2i} &amp;= \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d\\_model}}}\\right) \\\\ PE_{pos, 2i+1} &amp;= \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d\\_model}}}\\right) \\end{align*} \\right. $$</p> <p>For each dimension, a different frequency is used to ensure that the encodings are distinct from each other. The following animation illustrates this concept:</p> Positional Encoding Animation <p>For more details about the animation check the following Tutorial: How positional encoding in transformers works?</p> <p>Now, let's see positional encoding in pytorch code \ud83d\ude01.</p> <p>First of all, we need to import the <code>PyTorch</code> framework as well as <code>numpy</code> for numerical computation, the <code>matplotlib</code> and <code>seaborn</code> for making some plots later.</p> In\u00a0[5]: Copied! <pre>import torch, torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n</pre> import torch, torch.nn as nn import matplotlib.pyplot as plt import seaborn as sns import numpy as np <p>As has been said Positional Encoding is represented by matrix $PE \\in \\mathbb{R}^{n \\times d\\_model}$, where $n$ represents the context size, and the $d\\_model$ represents the embedding dimension.</p> In\u00a0[4]: Copied! <pre>block_size = 1024 # represent the context size &lt;=&gt; n.\nd_model = 512\n</pre> block_size = 1024 # represent the context size &lt;=&gt; n. d_model = 512 <p>For instance, let's initialize the <code>pe</code> matrix of zeros, it has the shape of <code>block_size</code>, which represents the context size, and finally <code>d_model</code>.</p> In\u00a0[45]: Copied! <pre>pe = torch.zeros(block_size, d_model)\npe[:5]\n</pre> pe = torch.zeros(block_size, d_model) pe[:5] Out[45]: <pre>tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])</pre> In\u00a0[46]: Copied! <pre>pe.shape\n</pre> pe.shape Out[46]: <pre>torch.Size([1024, 512])</pre> <p>We remind you of the formula, with some changes in which we set <code>f_i</code> as the frequency. $$ \\forall i \\in \\left[0, \\frac{d\\_model}{2}\\right] \\left\\{ \\begin{align*} PE_{pos, 2i} &amp;= \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d\\_model}}}\\right) &amp;= \\hspace{0.2cm} \\sin\\left(f_i pos\\right)\\\\ PE_{pos, 2i+1} &amp;= \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d\\_model}}}\\right)  &amp;=  \\hspace{0.2cm} \\cos\\left(f_i pos\\right) \\end{align*} \\right. $$</p> <p>Where</p> <p>$$ \\forall i \\in \\left[0, \\frac{d\\_model}{2}\\right] \\mid f_i = \\frac{1}{10000^{\\frac{2i}{d\\_model}}} $$</p> <p>let's start with <code>pos</code> which ranges from 0 to the context size <code>block_size</code>.</p> In\u00a0[6]: Copied! <pre>pos = torch.arange(0, block_size).unsqueeze(1)\npos\n</pre> pos = torch.arange(0, block_size).unsqueeze(1) pos Out[6]: <pre>tensor([[   0],\n        [   1],\n        [   2],\n        ...,\n        [1021],\n        [1022],\n        [1023]])</pre> <p>For the implementation concerns, we're rewriting the frequency <code>f_i</code> formula, with <code>exp</code> and <code>log</code> functions.</p> <p>$$ \\forall i \\in \\left[0, \\frac{d\\_model}{2}\\right] \\mid f_i = \\frac{1}{10000^{\\frac{2i}{d\\_model}}} = \\exp\\left[-2i \\frac{\\log{10000}}{d\\_model}\\right] $$</p> In\u00a0[179]: Copied! <pre>freq = torch.exp(\n    - torch.arange(0, d_model, 2) * torch.log(torch.Tensor([10000])) / d_model\n)\nprint(\"freq shape:\", freq.shape)\nfreq[:50]\n</pre> freq = torch.exp(     - torch.arange(0, d_model, 2) * torch.log(torch.Tensor([10000])) / d_model ) print(\"freq shape:\", freq.shape) freq[:50] <pre>freq shape: torch.Size([256])\n</pre> Out[179]: <pre>tensor([1.0000, 0.9647, 0.9306, 0.8977, 0.8660, 0.8354, 0.8058, 0.7774, 0.7499,\n        0.7234, 0.6978, 0.6732, 0.6494, 0.6264, 0.6043, 0.5829, 0.5623, 0.5425,\n        0.5233, 0.5048, 0.4870, 0.4698, 0.4532, 0.4371, 0.4217, 0.4068, 0.3924,\n        0.3786, 0.3652, 0.3523, 0.3398, 0.3278, 0.3162, 0.3051, 0.2943, 0.2839,\n        0.2738, 0.2642, 0.2548, 0.2458, 0.2371, 0.2288, 0.2207, 0.2129, 0.2054,\n        0.1981, 0.1911, 0.1843, 0.1778, 0.1715])</pre> <p>The shape of <code>freq</code> is equal to $\\frac{d\\_model}{2}$, because in half of the dimensions we use <code>cos</code> (for odd dimensions), the rest <code>sin</code> is used.</p> In\u00a0[181]: Copied! <pre>pos_freq = pos * freq\nprint(pos_freq.shape)\npos_freq\n</pre> pos_freq = pos * freq print(pos_freq.shape) pos_freq  <pre>torch.Size([1024, 256])\n</pre> Out[181]: <pre>tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n         0.0000e+00],\n        [1.0000e+00, 9.6466e-01, 9.3057e-01,  ..., 1.1140e-04, 1.0746e-04,\n         1.0366e-04],\n        [2.0000e+00, 1.9293e+00, 1.8611e+00,  ..., 2.2279e-04, 2.1492e-04,\n         2.0733e-04],\n        ...,\n        [1.0210e+03, 9.8492e+02, 9.5011e+02,  ..., 1.1374e-01, 1.0972e-01,\n         1.0584e-01],\n        [1.0220e+03, 9.8588e+02, 9.5104e+02,  ..., 1.1385e-01, 1.0982e-01,\n         1.0594e-01],\n        [1.0230e+03, 9.8685e+02, 9.5198e+02,  ..., 1.1396e-01, 1.0993e-01,\n         1.0605e-01]])</pre> <p>Now we select all rows, with each row representing a token position. For all even dimensions, we map the <code>sin</code> function to <code>pos_freq</code>, and for odd dimensions, we map the <code>cos</code> function.</p> In\u00a0[52]: Copied! <pre>pe[:, 0::2] = torch.sin(pos_freq)\npe[:, 1::2] = torch.cos(pos_freq)\n</pre> pe[:, 0::2] = torch.sin(pos_freq) pe[:, 1::2] = torch.cos(pos_freq) In\u00a0[182]: Copied! <pre>print(pe.shape)\npe\n</pre> print(pe.shape) pe <pre>torch.Size([1024, 512])\n</pre> Out[182]: <pre>tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n          0.0000e+00,  1.0000e+00],\n        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n          1.0366e-04,  1.0000e+00],\n        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n          2.0733e-04,  1.0000e+00],\n        ...,\n        [ 1.7612e-02, -9.9984e-01, -9.9954e-01,  ...,  9.9399e-01,\n          1.0564e-01,  9.9440e-01],\n        [-8.3182e-01, -5.5504e-01, -5.4462e-01,  ...,  9.9398e-01,\n          1.0575e-01,  9.9439e-01],\n        [-9.1649e-01,  4.0007e-01,  3.7901e-01,  ...,  9.9396e-01,\n          1.0585e-01,  9.9438e-01]])</pre> In\u00a0[183]: Copied! <pre>sns.heatmap(pe.numpy())\n</pre> sns.heatmap(pe.numpy()) Out[183]: <pre>&lt;Axes: &gt;</pre> <p>Let's create the <code>PositionalEncoding</code> PyTorch module that combines everything we've discussed previously.</p> <p>The <code>PositionalEncoding</code> module defines a <code>forward</code> function, which accepts an input along with the corresponding position encoding.</p> In\u00a0[184]: Copied! <pre>class PositionalEncoding(nn.Module):\n\n\n    def __init__(self, block_size: int = 20, d_model: int = 1):\n        super(PositionalEncoding, self).__init__()\n        \n        self.block_size: int = block_size\n        self.d_model: int = d_model\n\n        pe = torch.zeros(self.block_size, self.d_model)\n        \n        pos = torch.arange(0, self.block_size).unsqueeze(1) \n\n        freq = torch.exp(\n            - torch.arange(0, self.d_model, 2) * torch.log(torch.Tensor([10000])) / self.d_model\n        )\n\n        pe[:, 0::2] = torch.sin(pos * freq) # for Even positions\n        pe[:, 1::2] = torch.cos(pos * freq) # for Odd positions\n        \n        self.register_buffer('pe', pe)\n        \n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n\n        return input + self.pe[:, :input.size(1)]\n</pre> class PositionalEncoding(nn.Module):       def __init__(self, block_size: int = 20, d_model: int = 1):         super(PositionalEncoding, self).__init__()                  self.block_size: int = block_size         self.d_model: int = d_model          pe = torch.zeros(self.block_size, self.d_model)                  pos = torch.arange(0, self.block_size).unsqueeze(1)           freq = torch.exp(             - torch.arange(0, self.d_model, 2) * torch.log(torch.Tensor([10000])) / self.d_model         )          pe[:, 0::2] = torch.sin(pos * freq) # for Even positions         pe[:, 1::2] = torch.cos(pos * freq) # for Odd positions                  self.register_buffer('pe', pe)              def forward(self, input: torch.Tensor) -&gt; torch.Tensor:          return input + self.pe[:, :input.size(1)] <p>Now, let's create an instance of the <code>PositionalEncoding</code> module. We will set the context size to 100 and the embedding dimension (<code>d_model</code>) to 20.</p> In\u00a0[185]: Copied! <pre>pos_encoding = PositionalEncoding(\n                block_size = 100,\n                d_model= 20\n            )\n</pre> pos_encoding = PositionalEncoding(                 block_size = 100,                 d_model= 20             ) In\u00a0[186]: Copied! <pre>pos_encoding\n</pre> pos_encoding Out[186]: <pre>PositionalEncoding()</pre> <p>To test the positional encoding object, we'll first create an <code>input</code> tensor filled with zeros, having a shape of <code>(batch = 1, block_size = 100, d_model = 20)</code>.</p> In\u00a0[187]: Copied! <pre>input = torch.zeros(1, 100, 20)\n</pre> input = torch.zeros(1, 100, 20) In\u00a0[188]: Copied! <pre>out = pos_encoding(input)\nout\n</pre> out = pos_encoding(input) out Out[188]: <pre>tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n           0.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  5.4030e-01,  3.8767e-01,  ...,  1.0000e+00,\n           2.5119e-04,  1.0000e+00],\n         [ 9.0930e-01, -4.1615e-01,  7.1471e-01,  ...,  1.0000e+00,\n           5.0238e-04,  1.0000e+00],\n         ...,\n         [ 3.7961e-01, -9.2515e-01,  7.9395e-01,  ...,  9.9813e-01,\n           2.4363e-02,  9.9970e-01],\n         [-5.7338e-01, -8.1929e-01,  9.6756e-01,  ...,  9.9809e-01,\n           2.4614e-02,  9.9970e-01],\n         [-9.9921e-01,  3.9821e-02,  9.8984e-01,  ...,  9.9805e-01,\n           2.4865e-02,  9.9969e-01]]])</pre> In\u00a0[189]: Copied! <pre>out.shape\n</pre> out.shape Out[189]: <pre>torch.Size([1, 100, 20])</pre> <p>The plot shows selected dimensions of the positional encoding, illustrating their periodic patterns. It visualizes dimensions 2, 4, 6, and 8, highlighting how they oscillate with different frequencies.</p> In\u00a0[190]: Copied! <pre>plt.figure(figsize=(15, 5))\nplt.plot(np.arange(100), out[0, :, 2:10:2].data.numpy())\nplt.legend([\"dim %d\"%p for p in range(2, 10, 2)])\n</pre> plt.figure(figsize=(15, 5)) plt.plot(np.arange(100), out[0, :, 2:10:2].data.numpy()) plt.legend([\"dim %d\"%p for p in range(2, 10, 2)]) Out[190]: <pre>&lt;matplotlib.legend.Legend at 0x7d100ca19890&gt;</pre> <p>A technical detail here: we've discussed how to compute the positional encoding table using periodic functions <code>cos</code> and <code>sin</code>. However, this is not critical. It can be set as an Embedding table and learned from the data. In the original paper, Attention Is All You Need, the authors mention that in their experiments, they set learnable positional embeddings and achieved nearly identical results.</p> Learnable Positional Encoding"},{"location":"llms/attention-is-all-you-need/#attention-is-all-you-need","title":"Attention Is All You Need\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#introduction","title":"Introduction\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#01-whats-motivation-behind-transformers","title":"01. What's Motivation behind Transformers ?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#02-preliminaries","title":"02. Preliminaries\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#21-understanding-tokenization-in-language-modeling","title":"2.1 Understanding Tokenization in Language Modeling\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#whats-tokenization","title":"What's Tokenization ?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#why-tokenization-is-important","title":"Why Tokenization is important?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#tokenization-in-code","title":"Tokenization in code\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#22-understanding-embeddings","title":"2.2 Understanding Embeddings\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#why-are-embeddings-important","title":"Why are Embeddings important?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#how-are-embeddings-created","title":"How are Embeddings Created?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#embeddings-in-code","title":"Embeddings in code\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#23-from-tokenization-to-embeddings","title":"2.3 From Tokenization to Embeddings\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#24-understanding-weighted-aggregation","title":"2.4 Understanding Weighted Aggregation\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#whats-aggregation-in-machine-learning","title":"What's Aggregation in Machine Learning ?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#weighted-aggregation-an-inefficient-version","title":"Weighted Aggregation (an Inefficient version)\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#weighted-aggregation-an-effecient-way","title":"Weighted Aggregation (An effecient Way)\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#03-transformer-embedding-layer","title":"03. Transformer - Embedding Layer\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#31-what-an-embedding-layer-is","title":"3.1 What an Embedding Layer is ?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#32-embedding-layer-in-code","title":"3.2 Embedding Layer in Code\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#04-transformer-positional-encoding","title":"04. Transformer - Positional Encoding\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#41-whats-why-potional-encoding","title":"4.1 What's &amp; Why Potional Encoding ?\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#42-positional-encoding-in-code","title":"4.2 Positional Encoding in Code\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#43-positional-encoding-pytorch-module","title":"4.3 Positional Encoding PyTorch Module\u00b6","text":""},{"location":"llms/attention-is-all-you-need/#44-positional-encoding-with-nnembedding","title":"4.4 Positional Encoding with <code>nn.Embedding</code>\u00b6","text":""},{"location":"llms/rag-pytorch-from-scratch/","title":"Local RAG from scratch using PyTorch","text":"<p>This project uses PyTorch to build a local RAG pipeline from scratch.</p> <p>We're going to build a chat PDF system. That allows you to give any PDF of your choice, i.e. book, article, etc. Then ask any question and get a custom response.</p> <p>Excellent frameworks, such as LangChain and LamaIndex, facilitate work with LLMs and help build this kind of pipeline. But our goal is to create everything from scratch.</p> <p>RAG stands for Retrieval Autgmented Generation.</p> <p>The paper introduced it as Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks from Facebook AI research.</p> <p>Let's break RAG down:</p> <ul> <li>Retrieval - Seeking relevant information given a query within a database and, for example, getting relevant passages from Wikipedia texts when given a question.</li> <li>Augmented - Using the relevant retrieved information to modify the input (prompts) to a generative model (e.g., LLMs).</li> <li>Generation - Generating output given the augmented input; this is done using a generative model, e.g. LLMs.</li> </ul> <p>LLMs excel in language modeling, demonstrating a deep understanding of language. They can produce responses by leveraging the text they have been trained on. Although LLMs generate good text, it doesn't mean it is factual.</p> <p>So, let's cover why the RAG is essential for LLMs.</p> <ul> <li><p>Prevent Hallucination: LLMs are probabilistic models, which means they can give incorrect information. For some use cases, getting factual information is critical. This is when the RAG comes in; it helps with retrieving some facts about the user's query to improve the generation.</p> </li> <li><p>Custom Data: LLMs are first pre-trained to understand the language, and then fine-tuned to adapt to specific tasks. However, with this method, every time we receive new data we have to train the model again, which is time and money-consuming. RAG can help LLMs by providing them with relevant information without needing to be fine-tuned.</p> </li> </ul> <p>All the work is done locally. Why ?</p> <p>Privacy, Speed, and Cost. Let's break them down in detail:</p> <ul> <li>Privacy: locally we don't need to send sensitive data to an API, e.g. ChatGPT API.</li> <li>Speed: we won't have to wait for an API downtime. If our hardware running, the pipeline is running.</li> <li>Cost: using APIs costs money for every request you make. if you have your hardware you need to pay nothing.</li> </ul>"},{"location":"llms/rag-pytorch-from-scratch/#local-rag-from-scratch-using-pytorch","title":"Local RAG from scratch using PyTorch\u00b6","text":""},{"location":"llms/rag-pytorch-from-scratch/#01-retrieval-autgmented-generation-rag","title":"01. Retrieval Autgmented Generation (RAG)\u00b6","text":""},{"location":"llms/rag-pytorch-from-scratch/#11-whats-rag","title":"1.1 What's RAG?\u00b6","text":""},{"location":"llms/rag-pytorch-from-scratch/#12-why-rag-is-important","title":"1.2 Why RAG is important?\u00b6","text":""},{"location":"llms/rag-pytorch-from-scratch/#13-why-local","title":"1.3 Why Local?\u00b6","text":""},{"location":"papers/","title":"Papers Understanding and Replicating","text":""},{"location":"papers/graph_conv_net/graph_conv_net/","title":"Semi-Supervised Classification with GCN","text":"<p>Graph Convolutional Networks by Moussa Jamor</p> <p>Graph Convolutional Networks (GCN) is graph-base neural network architecture introduced by Thomas N. Kipf and Max Welling in their paper ti- tled Semi-Supervised Classification with Graph Convolutional Networks in 2017. As they mentioned in their paper, that this proposed architecture is an effecient variant of Convolution Neural Network (CNN), and It\u2019s the first- order approximation of spectral graph convoltions.</p> <p>In their paper [2], Thomas N. Kipf and Max Welling introduce the concept of multi-layer graph convolutional networks (GCNs). They propose a layer-wise propagation rule defined by the equation $A$:</p> <p>$$     H^{(l+1)} = \\sigma\\left(\\widetilde{D}^{-\\frac{1}{2}} \\widetilde{A} \\widetilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)}\\right) $$</p> <p>Neural networks are constructed with an input layer, hidden layers (often many), and an output layer. Hidden layers typically consist of linear layers (although not always) followed by an activation function to introduce non-linearity. The linear layer is defined as:</p> <p>$$     y = XW $$</p> <p>where $X$ is the feature matrix and $W$ is the learnable weight matrix updated during backpropagation. Since Graph Convolutional Networks (GCNs) are based on neural network architectures, this term is present in Equation:</p> <p>$$ \\begin{equation}     y = H^{(l)}W^{(l)} \\end{equation} $$</p> <p>where $H^{(l)}$ is obtained by stacking all feature vectors for each node horizontally, and $W^{(l)}$ is the learnable weight matrix.</p> <p></p> <p>Grpah and Node Embeddings</p> <p>The adjacency matrix is a square matrix A where its elements are defined by:</p> <p>$$     a_{ij} = \\begin{cases}                 1 &amp; \\text{if } v_i \\text{ and } v_j \\text{ are adjacent}\\\\                 0 &amp; \\text{otherwise}               \\end{cases} $$</p> <p>The adjacency matrix A has some properties:</p> <ul> <li><p>Squared Matrix: The adjacency matrix is always squared because each vertex is compared to all other vertices in the graph, resulting in a square matrix.</p> </li> <li><p>Symmetric Matrix: If the graph is undirected, the adjacency matrix is symmetric because if $v_i$ is adjacent to $v_j$ , then $v_j$ is also adjacent to $v_i$.</p> </li> </ul> <p>In Equation above, the authors use $\\widetilde{A}$, which is defined as:</p> <p>$$     \\widetilde{A} = A + I_N $$</p> <p>Here, $N$ denotes the number of nodes in the graph. The addition of $I_N$ to the adjacency matrix is done to add a self-connection for every node within the graph.</p> <p>The Degree Matrix is a diagonal matrix containing the number of edges attached to each vertex. Formally, it is defined as:</p> <p>$$ \\begin{equation}     \\widetilde{D}_{ij} = \\begin{cases}         d_i = \\deg(v_i) &amp; \\text{if }i=j \\\\         0 &amp; \\text{otherwise}      \\end{cases} \\end{equation} $$</p> <p>Alternatively, the $\\widetilde{D}$ matrix can be expressed using the $\\widetilde{A}$ adjacency matrix as:</p> <p>$$ \\begin{equation}     \\widetilde{D}_{ij} = \\begin{cases}         \\sum_{j}^{} \\widetilde{A}_{ij} &amp; \\text{if } i=j\\\\         0 &amp; \\text{otherwise}     \\end{cases} \\end{equation} $$</p> <p>So the matrix in compact form is as follows:</p> <p>$$ \\begin{equation}     \\widetilde{D} = \\begin{bmatrix}         d_1 = \\deg{v_1} &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\ddots &amp; &amp; \\ddots  &amp; \\vdots \\\\         0 &amp; \\cdots &amp; d_i = \\deg{v_i} &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\ddots  &amp;  &amp; \\ddots &amp; \\vdots \\\\         0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; d_N = \\deg{v_N} \\\\     \\end{bmatrix} \\end{equation} $$</p> <p>In first equation, we don't have $\\widetilde{D}$. Instead, we have $\\widetilde{D}^{-\\frac{1}{2}}$, which is defined as:</p> <p>$$ \\begin{equation}     \\widetilde{D}^{-\\frac{1}{2}}_{ij} = \\begin{cases}         \\frac{1}{\\sqrt{d_i}} &amp; \\text{if }i=j\\\\         0 &amp; \\text{otherwise}     \\end{cases} \\end{equation} $$</p> <p>Thus, the compact form is:</p> <p>$$ \\begin{equation}     \\widetilde{D}^{-\\frac{1}{2}} = \\begin{bmatrix}         \\frac{1}{\\sqrt{d_1}} &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\ddots &amp; &amp; \\ddots  &amp; \\vdots \\\\         0 &amp; \\cdots &amp; \\frac{1}{\\sqrt{d_i}} &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\ddots  &amp;  &amp; \\ddots &amp; \\vdots \\\\         0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; \\frac{1}{\\sqrt{d_N}}  \\\\     \\end{bmatrix} \\end{equation} $$</p> <p>The normalized adjacency matrix is denoted as $\\hat{A}$ and is defined as follows:</p> <p>$$ \\begin{equation}     \\hat{A} = \\widetilde{D}^{-\\frac{1}{2}}(A + I_N)\\widetilde{D}^{-\\frac{1}{2}} = \\widetilde{D}^{-\\frac{1}{2}}\\widetilde{A}\\widetilde{D}^{-\\frac{1}{2}} \\end{equation} $$</p> <p>Normalizing the adjacency matrix $\\widetilde{A}$ helps avoid numerical instabilities and issues such as exploding and vanishing gradients when used in a deep neural network model.</p> <p>The \\textbf{Message Passing} refers to the process updating node representation in graph-based, on information from neighboring nodes. This operation, is performed when multiply the normalized adjacency matrix $\\hat{A}$ with feature matrix $H^{(l)}$. Formally,</p> <p>$$ \\begin{equation}     X^{(l)} = \\hat{A}H^{(l)} = \\widetilde{D}^{-\\frac{1}{2}}\\widetilde{A}\\widetilde{D}^{-\\frac{1}{2}}H^{(l)} \\end{equation} $$</p> <p>After performing the Message Passing, we multiply the $X^{(l)}$ matrix with a trained weight matrix $W^{(l)}$, which is going to be updated during back-propagation. Finally, we apply a differentiable activation function $\\sigma$, which adds some sort of non-linearity to the network. So, the final result is:</p> <p>$$ \\begin{equation}     H^{(l+1)} = \\sigma\\left(\\hat{A}H^{(l)} W^{(l)}\\right) = \\sigma\\left(\\widetilde{D}^{-\\frac{1}{2}} \\widetilde{A} \\widetilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)}\\right) \\end{equation} $$</p>"},{"location":"papers/graph_conv_net/graph_conv_net/#semi-supervised-classification-with-gcn","title":"Semi-Supervised Classification with GCN\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#cgn-layers","title":"CGN Layers\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#linear-transformation-in-gcns","title":"Linear Transformation in GCNs\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#adjacency-matrix","title":"Adjacency Matrix\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#degree-matrix","title":"Degree Matrix\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#normalized-adjacency-matrix","title":"Normalized Adjacency Matrix\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#message-passing","title":"Message Passing\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#activation-function","title":"Activation Function\u00b6","text":""},{"location":"papers/graph_conv_net/graph_conv_net/#references","title":"References\u00b6","text":"<ul> <li><p>Thomas N. Kipf and Max Welling. : Semi-Supervised Classification with Graph Convolutional Networks, 2017.</p> </li> <li><p> Why do graph convolutional neural networks use normalized adjacency matrices?</p> </li> <li><p>Great Youtube Video: Graph Convolutional Networks (GCN) | GNN Paper Explained</p> </li> </ul>"},{"location":"papers/opt_methods/","title":"Introduction","text":""},{"location":"papers/opt_methods/adadelta/","title":"ADADELTA: An Adaptive Learning Rate Method","text":"<p>The link of the paper : https://arxiv.org/abs/1212.5701</p>"},{"location":"papers/opt_methods/adadelta/#adadelta-an-adaptive-learning-rate-method","title":"ADADELTA: An Adaptive Learning Rate Method\u00b6","text":""}]}